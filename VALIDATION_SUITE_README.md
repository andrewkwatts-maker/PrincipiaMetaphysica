# Principia Metaphysica - Validation Suite

## Overview

The comprehensive validation suite ensures code quality, correctness, and consistency across the entire Principia Metaphysica project. It is designed for both local development and CI/CD pipelines (GitHub Actions).

## Quick Start

### Run Full Validation

```bash
# Run all validations (simulations, tests, and checks)
python simulations/validate_all.py

# Run with verbose output (default)
python simulations/validate_all.py --verbose

# Run quietly (minimal output)
python simulations/validate_all.py --quiet
```

### Run Partial Validation

```bash
# Skip simulations (faster, useful for quick checks)
python simulations/validate_all.py --skip-simulations

# Skip pytest tests
python simulations/validate_all.py --skip-tests

# Skip both (only validate data structures)
python simulations/validate_all.py --skip-simulations --skip-tests
```

## What Gets Validated

The validation suite performs **7 comprehensive checks**:

### 1. Simulations Execution
- **What**: Runs `run_all_simulations.py` to execute all v16 simulations
- **Duration**: ~30-60 seconds
- **Validates**:
  - All simulations execute without errors
  - Dependencies are satisfied
  - Outputs are computed correctly
  - Theory output is generated

### 2. Pytest Tests
- **What**: Runs all unit tests in `tests/` directory
- **Duration**: ~5-10 seconds
- **Validates**:
  - Core module functionality
  - Physics calculations
  - Data structure integrity
  - Configuration management

### 3. Theory Output Structure
- **What**: Validates `AutoGenerated/theory_output.json` schema
- **Duration**: <1 second
- **Validates**:
  - Required top-level keys present:
    - `metadata`, `derivation_logic`, `parameter_classification`
    - `parameters`, `formulas`, `sections`, `provenance`, `validation`
  - Metadata contains version, timestamp, git info
  - File is valid JSON
  - Counts parameters, formulas, sections

### 4. Experimental Data Files
- **What**: Validates JSON files in `simulations/data/experimental/`
- **Duration**: <1 second
- **Validates**:
  - `pdg_2024_values.json` (PDG experimental constants)
  - `nufit_6_0_parameters.json` (NuFIT neutrino parameters)
  - `desi_2025_constraints.json` (DESI cosmology data)
  - All files are valid JSON
  - No parsing errors

### 5. Formula Metadata
- **What**: Validates all formulas in theory output have required metadata
- **Duration**: <1 second
- **Validates**:
  - **Required fields**: `latex`, `category`
  - **Recommended fields**: `description`, `domain`
  - No formulas missing critical metadata
  - Reports warnings for missing recommended fields

### 6. Parameter Provenance
- **What**: Validates all parameters have proper source tracking
- **Duration**: <1 second
- **Validates**:
  - **Required fields**: `value`, `source`
  - **Recommended fields**: `source_simulation`, `status`
  - Provenance entries exist for derived parameters
  - Parameter classification is correct

### 7. AutoGenerated JSON Files
- **What**: Validates all JSON files in `AutoGenerated/` directory
- **Duration**: <1 second
- **Validates**:
  - All files are valid JSON
  - No parsing errors in:
    - `theory_output.json`
    - `formulas.json`, `parameters.json`, `sections.json`
    - `beginner-guide.json`
    - Certificate files in `certificates/`
    - Derivation chains in `derivations/`

## Exit Codes

- **Exit 0**: All validations passed
- **Exit 1**: One or more validations failed

This makes the script perfect for CI/CD pipelines - the build will fail if validations don't pass.

## Output Files

### Validation Report JSON

The validation suite generates a detailed JSON report at:

```
simulations/validation_report.json
```

**Report Structure**:

```json
{
  "timestamp": "2025-12-29T10:54:48.752020",
  "duration_seconds": 0.06,
  "all_passed": true,
  "sections": {
    "Theory Output Structure": {
      "passed": true,
      "details": {
        "file_size_kb": 1234.5,
        "parameters_count": 176,
        "formulas_count": 91,
        "sections_count": 9,
        "provenance_entries": 150
      },
      "errors": [],
      "warnings": []
    },
    "...": "..."
  },
  "summary": {
    "total_checks": 7,
    "passed": 7,
    "failed": 0,
    "warnings": 91
  }
}
```

This report can be:
- Uploaded as a CI artifact
- Parsed by automated tools
- Used for regression tracking
- Displayed in dashboards

## GitHub Actions CI/CD

### Automatic Validation on Push

The validation suite runs automatically on:
- Every push to `main` or `develop` branches
- Every pull request to `main` or `develop`
- Manual workflow dispatch

### Workflow Configuration

The GitHub Actions workflow is defined in:

```
.github/workflows/validate.yml
```

### CI Jobs

#### 1. Full Validation (Matrix)
- **Python versions**: 3.11, 3.12
- **Runs**: All 7 validations
- **Duration**: ~5-10 minutes
- **Artifacts**: Validation report + theory output

#### 2. Fast Validation
- **Python version**: 3.12
- **Runs**: Validations 3-7 (skips simulations)
- **Duration**: ~1-2 minutes
- **Artifacts**: Validation report

#### 3. Summary
- **Runs after**: Both validation jobs
- **Generates**: GitHub Actions summary with pass/fail status
- **Downloads**: All validation reports

### Viewing CI Results

1. Go to the **Actions** tab in GitHub
2. Click on the latest workflow run
3. View the **Summary** for quick status
4. Download **Artifacts** for detailed reports
5. Check **Job logs** for full output

### Manual Workflow Trigger

You can manually trigger the validation workflow:

1. Go to **Actions** tab
2. Select "Principia Metaphysica - Validation Suite"
3. Click **Run workflow**
4. Choose branch and click **Run**

## Local Development Workflow

### Before Committing

```bash
# Run quick validation
python simulations/validate_all.py --skip-simulations

# If changes affect simulations, run full validation
python simulations/validate_all.py
```

### After Making Changes

**Modified core code**:
```bash
# Run pytest tests
python simulations/validate_all.py --skip-simulations
```

**Modified simulations**:
```bash
# Run full validation
python simulations/validate_all.py
```

**Modified data files**:
```bash
# Run quick validation
python simulations/validate_all.py --skip-simulations --skip-tests
```

### Debugging Failed Validations

**Check the validation report**:
```bash
cat simulations/validation_report.json | python -m json.tool
```

**Run specific validation manually**:
```bash
# Just run simulations
python run_all_simulations.py

# Just run pytest
python -m pytest tests/ -v

# Validate specific JSON file
python -m json.tool AutoGenerated/theory_output.json > /dev/null
```

## Integration with Other Tools

### Pre-commit Hooks

Add to `.git/hooks/pre-commit`:

```bash
#!/bin/bash
python simulations/validate_all.py --skip-simulations --quiet
exit $?
```

Make executable:
```bash
chmod +x .git/hooks/pre-commit
```

### VS Code Tasks

Add to `.vscode/tasks.json`:

```json
{
  "version": "2.0.0",
  "tasks": [
    {
      "label": "Validate All",
      "type": "shell",
      "command": "python",
      "args": ["simulations/validate_all.py"],
      "problemMatcher": []
    },
    {
      "label": "Quick Validate",
      "type": "shell",
      "command": "python",
      "args": ["simulations/validate_all.py", "--skip-simulations"],
      "problemMatcher": []
    }
  ]
}
```

### Make/Build Integration

Add to `Makefile`:

```makefile
.PHONY: validate validate-quick validate-full

validate-quick:
	python simulations/validate_all.py --skip-simulations

validate-full:
	python simulations/validate_all.py

validate: validate-full

test:
	python -m pytest tests/ -v

ci: validate test
```

## Customization

### Adding New Validations

To add a new validation check, edit `simulations/validate_all.py`:

```python
def validate_custom_check(self) -> None:
    """Validate custom requirement."""
    self.log("\n[X/7] Validating custom check...")
    self.log("-" * 80)

    errors = []
    warnings = []

    # Your validation logic here
    passed = len(errors) == 0

    details = {
        "custom_metric": 123
    }

    self.report.add_section("Custom Check", passed, details, errors, warnings)

    if passed:
        self.log("[OK] Custom check passed")
    else:
        self.log("[FAIL] Custom check failed")
```

Then add to `validate_all()` method:

```python
def validate_all(self) -> bool:
    # ... existing validations ...
    self.validate_custom_check()  # Add your check
    # ... rest of method ...
```

### Adjusting Validation Criteria

Edit the validation methods in `ComprehensiveValidator` class to change what gets checked.

For example, to make `domain` field required for formulas:

```python
# In validate_formula_metadata()
required_fields = ["latex", "category", "domain"]  # Add "domain"
```

## Performance

### Typical Execution Times

| Configuration | Duration | Use Case |
|--------------|----------|----------|
| Full validation | 30-60s | Pre-release, CI/CD |
| Skip simulations | 5-10s | Quick check, local dev |
| Skip tests + sims | <1s | Data structure only |

### Optimization Tips

1. **Local development**: Use `--skip-simulations` for faster feedback
2. **CI/CD**: Run full validation on `main`, fast on PRs
3. **Pre-commit**: Use `--skip-simulations --quiet`
4. **Debug**: Run specific checks manually instead of full suite

## Troubleshooting

### Common Issues

**Issue**: `theory_output.json not found`
- **Solution**: Run `python run_all_simulations.py` first

**Issue**: `pytest not installed`
- **Solution**: `pip install pytest pytest-cov`

**Issue**: Simulations timeout
- **Solution**: Increase timeout in `validate_all.py` (default: 300s)

**Issue**: JSON decode errors
- **Solution**: Check file encoding is UTF-8, validate JSON syntax

### Getting Help

1. Check validation report JSON for detailed error messages
2. Run failed component separately with verbose output
3. Review GitHub Actions logs for CI failures
4. Check git commit history for recent changes

## Best Practices

### For Contributors

1. **Always** run validation before committing
2. **Fix** all errors, investigate warnings
3. **Update tests** when adding features
4. **Document** new parameters and formulas properly

### For Maintainers

1. **Monitor** CI validation trends over time
2. **Review** validation reports in PRs
3. **Update** validation criteria as project evolves
4. **Archive** validation reports for major releases

## License

Copyright (c) 2025-2026 Andrew Keith Watts. All rights reserved.

Part of the Principia Metaphysica project.
