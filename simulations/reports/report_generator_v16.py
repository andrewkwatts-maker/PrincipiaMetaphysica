#!/usr/bin/env python3
"""
Technical Summary Report Generator v16.0
=========================================

Generates comprehensive technical summary reports from theory_output.json
suitable for journal submission as supplementary material.

This script:
1. Reads AutoGenerated/theory_output.json
2. Analyzes all parameters with uncertainties and sigma deviations
3. Generates formula derivations with validation status
4. Creates experimental comparison tables
5. Computes statistical summaries (chi-square, p-values)
6. Outputs to AutoGenerated/TECHNICAL_SUMMARY.md
7. Includes LaTeX-ready tables for paper appendix

The report is designed to be publication-quality supplementary material.

Usage:
    python simulations/reports/report_generator_v16.py

Output:
    AutoGenerated/TECHNICAL_SUMMARY.md

Copyright (c) 2025-2026 Andrew Keith Watts. All rights reserved.
"""

import json
import os
import sys
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
from collections import defaultdict
import math

# Add project root to path
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)


class TechnicalReportGenerator:
    """
    Generate comprehensive technical summary reports from theory_output.json.

    This class analyzes all parameters, formulas, and validation results,
    producing publication-quality supplementary material.
    """

    def __init__(self, theory_output_path: str = None, output_path: str = None):
        """
        Initialize the report generator.

        Args:
            theory_output_path: Path to theory_output.json (default: AutoGenerated/theory_output.json)
            output_path: Path for output markdown (default: AutoGenerated/TECHNICAL_SUMMARY.md)
        """
        if theory_output_path is None:
            theory_output_path = os.path.join(project_root, 'AutoGenerated', 'theory_output.json')
        if output_path is None:
            output_path = os.path.join(project_root, 'AutoGenerated', 'TECHNICAL_SUMMARY.md')

        self.theory_output_path = theory_output_path
        self.output_path = output_path
        self.data: Dict[str, Any] = {}

    def load_data(self) -> None:
        """Load theory_output.json with UTF-8 encoding."""
        print(f"Loading data from {self.theory_output_path}...")
        with open(self.theory_output_path, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        print(f"  Loaded {len(self.data.get('parameters', {}))} parameters")
        print(f"  Loaded {len(self.data.get('formulas', {}))} formulas")
        print(f"  Loaded {len(self.data.get('sections', []))} sections")

    def categorize_parameters(self) -> Dict[str, List[Tuple[str, Dict[str, Any]]]]:
        """
        Categorize parameters by status.

        Returns:
            Dictionary mapping status -> list of (path, param_data) tuples
        """
        categories = defaultdict(list)

        params = self.data.get('parameters', {})
        for path, param_data in params.items():
            status = param_data.get('status', 'UNKNOWN')
            categories[status].append((path, param_data))

        return dict(categories)

    def categorize_formulas(self) -> Dict[str, List[Tuple[str, Dict[str, Any]]]]:
        """
        Categorize formulas by category.

        Returns:
            Dictionary mapping category -> list of (formula_id, formula_data) tuples
        """
        categories = defaultdict(list)

        formulas = self.data.get('formulas', {})
        for formula_id, formula_data in formulas.items():
            category = formula_data.get('category', 'UNKNOWN')
            categories[category].append((formula_id, formula_data))

        return dict(categories)

    def compute_sigma_deviation(self, value: float, experimental: float,
                               uncertainty: Optional[float] = None) -> Optional[float]:
        """
        Compute sigma deviation from experimental value.

        Args:
            value: Theoretical prediction
            experimental: Experimental measurement
            uncertainty: Experimental uncertainty (if available)

        Returns:
            Number of sigma deviations, or None if uncertainty unavailable
        """
        if uncertainty is None or uncertainty == 0:
            return None

        return abs(value - experimental) / uncertainty

    def compute_chi_square(self, params: List[Tuple[str, Dict[str, Any]]]) -> Tuple[float, int, float]:
        """
        Compute chi-square statistic for parameters with experimental bounds.

        Args:
            params: List of (path, param_data) tuples

        Returns:
            Tuple of (chi_square, n_dof, p_value)
        """
        chi_square = 0.0
        n_dof = 0

        for path, param_data in params:
            value = param_data.get('value')
            exp_bound = param_data.get('experimental_bound')
            uncertainty = param_data.get('uncertainty')

            # Only include if we have experimental comparison with uncertainty
            if value is not None and exp_bound is not None and uncertainty and uncertainty > 0:
                deviation = (value - exp_bound) / uncertainty
                chi_square += deviation ** 2
                n_dof += 1

        # Compute p-value using chi-square distribution (rough approximation)
        if n_dof > 0:
            # For large n_dof, chi^2 approaches normal distribution
            # p-value ≈ 1 - erf((chi^2 - n_dof) / sqrt(2*n_dof))
            # Simplified: p ≈ exp(-(chi^2 - n_dof)^2 / (2*n_dof))
            if chi_square > n_dof:
                p_value = math.exp(-((chi_square - n_dof) ** 2) / (2 * n_dof))
            else:
                p_value = 1.0
        else:
            p_value = 1.0

        return chi_square, n_dof, p_value

    def format_value(self, value: Any, units: str = "") -> str:
        """
        Format a value for display.

        Args:
            value: Parameter value
            units: Units string

        Returns:
            Formatted string
        """
        if value is None:
            return "N/A"

        # Handle dict and list values
        if isinstance(value, (dict, list)):
            return "[Complex]"

        if isinstance(value, (int, float)):
            if abs(value) > 1e4 or (abs(value) < 1e-3 and value != 0):
                formatted = f"{value:.3e}"
            else:
                formatted = f"{value:.6g}"
        else:
            formatted = str(value)

        if units:
            formatted += f" {units}"

        return formatted

    def format_uncertainty(self, value: float, uncertainty: Optional[float], units: str = "") -> str:
        """
        Format value with uncertainty.

        Args:
            value: Central value
            uncertainty: Uncertainty
            units: Units string

        Returns:
            Formatted string like "1.23 ± 0.05 GeV"
        """
        # Handle non-numeric values
        if not isinstance(value, (int, float)):
            return self.format_value(value, units)

        if uncertainty is None:
            return self.format_value(value, units)

        # Determine appropriate precision based on uncertainty
        if uncertainty == 0:
            return self.format_value(value, units)

        # Scientific notation if needed
        if abs(value) > 1e4 or (abs(value) < 1e-3 and value != 0):
            val_str = f"{value:.3e}"
            unc_str = f"{uncertainty:.2e}"
        else:
            # Determine decimal places from uncertainty
            if uncertainty >= 1:
                precision = 2
            else:
                precision = -int(math.floor(math.log10(abs(uncertainty)))) + 1
            val_str = f"{value:.{precision}f}"
            unc_str = f"{uncertainty:.{precision}f}"

        if units:
            return f"{val_str} ± {unc_str} {units}"
        else:
            return f"{val_str} ± {unc_str}"

    def generate_parameter_table(self, params: List[Tuple[str, Dict[str, Any]]],
                                 title: str) -> str:
        """
        Generate markdown table for parameters.

        Args:
            params: List of (path, param_data) tuples
            title: Table title

        Returns:
            Markdown formatted table
        """
        if not params:
            return f"\n### {title}\n\nNo parameters in this category.\n"

        md = [f"\n### {title}\n"]
        md.append(f"\n**Count:** {len(params)}\n")
        md.append("\n| Parameter | Value | Units | Source | Status |\n")
        md.append("|-----------|-------|-------|--------|--------|\n")

        for path, param_data in sorted(params, key=lambda x: x[0]):
            value = param_data.get('value')
            uncertainty = param_data.get('uncertainty')
            units = param_data.get('metadata', {}).get('units', '')
            source = param_data.get('source', 'N/A')
            status = param_data.get('status', 'N/A')

            # Format value with uncertainty
            value_str = self.format_uncertainty(value, uncertainty)

            md.append(f"| `{path}` | {value_str} | {units} | {source} | {status} |\n")

        return ''.join(md)

    def generate_parameter_table_latex(self, params: List[Tuple[str, Dict[str, Any]]],
                                       title: str) -> str:
        """
        Generate LaTeX table for parameters.

        Args:
            params: List of (path, param_data) tuples
            title: Table title

        Returns:
            LaTeX formatted table
        """
        if not params:
            return ""

        latex = ["\n\\begin{table}[htbp]\n"]
        latex.append("\\centering\n")
        latex.append("\\caption{" + title.replace('_', '\\_') + "}\n")
        latex.append("\\begin{tabular}{llllll}\n")
        latex.append("\\hline\n")
        latex.append("Parameter & Value & Uncertainty & Units & Source & Status \\\\\n")
        latex.append("\\hline\n")

        for path, param_data in sorted(params, key=lambda x: x[0]):
            value = param_data.get('value')
            uncertainty = param_data.get('uncertainty')
            units = param_data.get('metadata', {}).get('units', '')
            source = param_data.get('source', 'N/A')
            status = param_data.get('status', 'N/A')

            # Escape LaTeX special characters
            path_latex = path.replace('_', '\\_')
            source_latex = source.replace('_', '\\_')
            units_latex = units.replace('_', '\\_')

            # Format values
            if value is not None and isinstance(value, (int, float)):
                if abs(value) > 1e4 or (abs(value) < 1e-3 and value != 0):
                    val_str = f"${value:.3e}$"
                else:
                    val_str = f"${value:.6g}$"
            elif isinstance(value, (dict, list)):
                val_str = "[Complex]"
            else:
                val_str = "N/A"

            if uncertainty is not None and uncertainty != 0:
                if abs(uncertainty) > 1e4 or (abs(uncertainty) < 1e-3 and uncertainty != 0):
                    unc_str = f"${uncertainty:.2e}$"
                else:
                    unc_str = f"${uncertainty:.4g}$"
            else:
                unc_str = "---"

            latex.append(f"{path_latex} & {val_str} & {unc_str} & {units_latex} & {source_latex} & {status} \\\\\n")

        latex.append("\\hline\n")
        latex.append("\\end{tabular}\n")
        latex.append("\\end{table}\n")

        return ''.join(latex)

    def generate_experimental_comparison_table(self) -> str:
        """
        Generate table comparing predictions with experimental bounds.

        Returns:
            Markdown formatted table
        """
        md = ["\n## Experimental Comparisons\n"]
        md.append("\nThis table compares theoretical predictions with experimental measurements and bounds.\n")
        md.append("\n| Parameter | Theory | Experiment | Uncertainty | Sigma | Status |\n")
        md.append("|-----------|--------|------------|-------------|-------|--------|\n")

        params = self.data.get('parameters', {})
        comparisons = []

        for path, param_data in params.items():
            value = param_data.get('value')
            exp_bound = param_data.get('experimental_bound')
            uncertainty = param_data.get('uncertainty')
            bound_type = param_data.get('bound_type', '')

            if exp_bound is not None and value is not None:
                # Compute sigma deviation
                sigma = self.compute_sigma_deviation(value, exp_bound, uncertainty)

                # Determine status
                if bound_type == 'lower':
                    status = "PASS" if value > exp_bound else "FAIL"
                elif bound_type == 'upper':
                    status = "PASS" if value < exp_bound else "FAIL"
                elif bound_type == 'measured':
                    if sigma is not None:
                        if sigma < 1:
                            status = "EXCELLENT"
                        elif sigma < 2:
                            status = "GOOD"
                        elif sigma < 3:
                            status = "ACCEPTABLE"
                        else:
                            status = "POOR"
                    else:
                        status = "N/A"
                else:
                    status = "N/A"

                units = param_data.get('metadata', {}).get('units', '')

                comparisons.append({
                    'path': path,
                    'theory': self.format_value(value, units),
                    'experiment': self.format_value(exp_bound, units),
                    'uncertainty': self.format_value(uncertainty, units) if uncertainty else "N/A",
                    'sigma': f"{sigma:.2f}" if sigma is not None else "N/A",
                    'status': status
                })

        # Sort by sigma deviation (most significant first)
        comparisons.sort(key=lambda x: float(x['sigma']) if x['sigma'] != "N/A" else float('inf'))

        for comp in comparisons:
            md.append(f"| `{comp['path']}` | {comp['theory']} | {comp['experiment']} | "
                     f"{comp['uncertainty']} | {comp['sigma']} | {comp['status']} |\n")

        if not comparisons:
            md.append("| No experimental comparisons available | | | | | |\n")

        return ''.join(md)

    def generate_formula_summary(self) -> str:
        """
        Generate summary of formulas by category.

        Returns:
            Markdown formatted summary
        """
        md = ["\n## Formula Summary\n"]

        formula_categories = self.categorize_formulas()

        md.append("\n### Formulas by Category\n\n")

        for category in ['ESTABLISHED', 'THEORY', 'DERIVED', 'PREDICTIONS']:
            formulas = formula_categories.get(category, [])
            md.append(f"\n#### {category} ({len(formulas)} formulas)\n\n")

            if formulas:
                for formula_id, formula_data in sorted(formulas, key=lambda x: x[0]):
                    label = formula_data.get('label', '')
                    description = formula_data.get('description', '')

                    # Get validation status
                    validation = formula_data.get('validation', {})
                    is_validated = validation.get('validated', False)

                    status_icon = "✓" if is_validated else "○"

                    md.append(f"- **{status_icon} {formula_id}** {label}: {description}\n")

                    # Show inputs and outputs
                    input_params = formula_data.get('inputParams', [])
                    output_params = formula_data.get('outputParams', [])

                    if input_params:
                        md.append(f"  - Inputs: {', '.join(f'`{p}`' for p in input_params)}\n")
                    if output_params:
                        md.append(f"  - Outputs: {', '.join(f'`{p}`' for p in output_params)}\n")

        return ''.join(md)

    def generate_statistical_summary(self) -> str:
        """
        Generate statistical summary including chi-square analysis.

        Returns:
            Markdown formatted summary
        """
        md = ["\n## Statistical Summary\n"]

        param_categories = self.categorize_parameters()

        # Get all parameters with experimental comparisons
        all_params = []
        for params in param_categories.values():
            all_params.extend(params)

        # Filter to only those with experimental bounds
        experimental_params = [
            (path, data) for path, data in all_params
            if data.get('experimental_bound') is not None
        ]

        md.append(f"\n**Total Parameters:** {len(all_params)}\n")
        md.append(f"**Parameters with Experimental Data:** {len(experimental_params)}\n\n")

        # Compute chi-square
        chi_square, n_dof, p_value = self.compute_chi_square(experimental_params)

        md.append("### Chi-Square Analysis\n\n")
        md.append(f"- **χ² statistic:** {chi_square:.3f}\n")
        md.append(f"- **Degrees of freedom:** {n_dof}\n")
        md.append(f"- **χ²/DOF:** {chi_square/n_dof:.3f}\n" if n_dof > 0 else "- **χ²/DOF:** N/A\n")
        md.append(f"- **p-value:** {p_value:.6f}\n")

        # Interpretation
        md.append("\n**Interpretation:**\n")
        if n_dof == 0:
            md.append("- No parameters with experimental uncertainties available for chi-square test.\n")
        elif p_value > 0.05:
            md.append(f"- Good agreement with experimental data (p > 0.05)\n")
        elif p_value > 0.01:
            md.append(f"- Acceptable agreement with experimental data (0.01 < p < 0.05)\n")
        else:
            md.append(f"- Poor agreement with experimental data (p < 0.01)\n")

        # Parameter breakdown by status
        md.append("\n### Parameter Breakdown by Status\n\n")
        md.append("| Status | Count | Percentage |\n")
        md.append("|--------|-------|------------|\n")

        total = len(all_params)
        for status in ['ESTABLISHED', 'GEOMETRIC', 'DERIVED', 'PREDICTED', 'CALIBRATED']:
            count = len(param_categories.get(status, []))
            percentage = (count / total * 100) if total > 0 else 0
            md.append(f"| {status} | {count} | {percentage:.1f}% |\n")

        return ''.join(md)

    def generate_metadata_summary(self) -> str:
        """
        Generate summary of metadata and provenance.

        Returns:
            Markdown formatted summary
        """
        md = ["\n## Metadata and Provenance\n"]

        metadata = self.data.get('metadata', {})

        md.append(f"\n**Generated:** {metadata.get('generated', 'N/A')}\n")
        md.append(f"**Version:** {metadata.get('version', 'N/A')}\n")
        md.append(f"**Git Commit:** {metadata.get('git_commit', 'N/A')}\n")

        # Provenance summary
        provenance = self.data.get('provenance', {})

        md.append("\n### Data Sources\n\n")
        md.append("| Source Type | Description |\n")
        md.append("|-------------|-------------|\n")

        if 'sources' in provenance:
            for source in provenance['sources']:
                source_type = source.get('type', 'N/A')
                description = source.get('description', 'N/A')
                md.append(f"| {source_type} | {description} |\n")

        return ''.join(md)

    def generate_latex_appendix(self) -> str:
        """
        Generate complete LaTeX appendix with all tables.

        Returns:
            LaTeX formatted appendix
        """
        latex = ["\n## LaTeX Tables for Paper Appendix\n"]
        latex.append("\nThe following LaTeX code can be copied directly into the paper appendix.\n")
        latex.append("\n```latex\n")
        latex.append("% =========================================\n")
        latex.append("% PRINCIPIA METAPHYSICA - PARAMETER TABLES\n")
        latex.append("% Auto-generated by report_generator_v16.py\n")
        latex.append(f"% Generated: {datetime.now().isoformat()}\n")
        latex.append("% =========================================\n\n")

        param_categories = self.categorize_parameters()

        # Generate table for each category
        for status in ['ESTABLISHED', 'GEOMETRIC', 'DERIVED', 'PREDICTED']:
            params = param_categories.get(status, [])
            if params:
                title = f"{status.title()} Parameters"
                latex.append(self.generate_parameter_table_latex(params, title))
                latex.append("\n")

        latex.append("```\n")

        return ''.join(latex)

    def generate_report(self) -> str:
        """
        Generate complete technical summary report.

        Returns:
            Complete markdown report
        """
        report = []

        # Header
        report.append("# Principia Metaphysica: Technical Summary Report\n")
        report.append(f"\n**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n")
        report.append(f"**Version:** v16.0\n")
        report.append("\n---\n")

        # Executive Summary
        report.append("\n## Executive Summary\n")
        report.append("\nThis technical summary provides comprehensive documentation of all parameters, ")
        report.append("formulas, and experimental validations in the Principia Metaphysica framework. ")
        report.append("It is designed as supplementary material for journal submission.\n")

        params = self.data.get('parameters', {})
        formulas = self.data.get('formulas', {})

        report.append(f"\n- **Total Parameters:** {len(params)}\n")
        report.append(f"- **Total Formulas:** {len(formulas)}\n")
        report.append(f"- **Sections:** {len(self.data.get('sections', []))}\n")

        # Table of Contents
        report.append("\n## Table of Contents\n")
        report.append("\n1. [Statistical Summary](#statistical-summary)\n")
        report.append("2. [Experimental Comparisons](#experimental-comparisons)\n")
        report.append("3. [Parameter Tables](#parameter-tables)\n")
        report.append("4. [Formula Summary](#formula-summary)\n")
        report.append("5. [Metadata and Provenance](#metadata-and-provenance)\n")
        report.append("6. [LaTeX Tables for Paper Appendix](#latex-tables-for-paper-appendix)\n")

        # Statistical Summary
        report.append(self.generate_statistical_summary())

        # Experimental Comparisons
        report.append(self.generate_experimental_comparison_table())

        # Parameter Tables
        report.append("\n## Parameter Tables\n")
        report.append("\nComplete listing of all parameters organized by status.\n")

        param_categories = self.categorize_parameters()

        for status in ['ESTABLISHED', 'GEOMETRIC', 'DERIVED', 'PREDICTED', 'CALIBRATED']:
            params = param_categories.get(status, [])
            if params:
                title = f"{status.title()} Parameters"
                report.append(self.generate_parameter_table(params, title))

        # Formula Summary
        report.append(self.generate_formula_summary())

        # Metadata
        report.append(self.generate_metadata_summary())

        # LaTeX Appendix
        report.append(self.generate_latex_appendix())

        # Footer
        report.append("\n---\n")
        report.append("\n## Report Generation Details\n")
        report.append(f"\n- **Source Data:** `{self.theory_output_path}`\n")
        report.append(f"- **Generated By:** `report_generator_v16.py`\n")
        report.append(f"- **Timestamp:** {datetime.now().isoformat()}\n")
        report.append("\n*This report is auto-generated. Do not edit manually.*\n")

        return ''.join(report)

    def save_report(self, report: str) -> None:
        """
        Save report to output file.

        Args:
            report: Report content
        """
        # Ensure output directory exists
        output_dir = os.path.dirname(self.output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        print(f"\nSaving report to {self.output_path}...")
        with open(self.output_path, 'w', encoding='utf-8') as f:
            f.write(report)

        # Get file size
        file_size = os.path.getsize(self.output_path)
        print(f"  Report saved: {file_size:,} bytes")

    def run(self) -> None:
        """Execute the complete report generation pipeline."""
        print("\n" + "="*60)
        print("Principia Metaphysica - Technical Summary Report Generator")
        print("="*60 + "\n")

        # Load data
        self.load_data()

        # Generate report
        print("\nGenerating technical summary...")
        report = self.generate_report()

        # Save report
        self.save_report(report)

        print("\n" + "="*60)
        print("Report generation complete!")
        print("="*60 + "\n")
        print(f"Output: {self.output_path}")
        print("\nThis report is ready for use as supplementary material")
        print("in journal submissions.\n")


def main():
    """Main entry point for standalone execution."""
    generator = TechnicalReportGenerator()
    generator.run()


if __name__ == '__main__':
    main()
