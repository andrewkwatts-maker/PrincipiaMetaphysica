#!/usr/bin/env python3
"""
Unified Output Validator v16.2
==============================

Validates Principia Metaphysica theory predictions against experimental data.

This validator reads ALL values from files - NO hardcoded predictions:
- Theory predictions from: AutoGenerated/theory_output.json
- Experimental values from: simulations/data/experimental/*.json
- Schema definitions from: validation_schema_v16_2.py

SOLID Principles:
- Single Responsibility: Only performs validation, doesn't compute predictions
- Open/Closed: New parameters added via schema, not by modifying validator
- Dependency Inversion: Depends on abstractions (schema, file paths), not concrete values

Copyright (c) 2025-2026 Andrew Keith Watts. All rights reserved.
"""

import json
import os
import sys
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path


# =============================================================================
# STRICT VALIDATION EXCEPTIONS
# =============================================================================

class ValidationError(Exception):
    """Base exception for validation failures."""
    pass


class MissingDataError(ValidationError):
    """Raised when required experimental or theory data is missing."""
    pass


class InvalidDataError(ValidationError):
    """Raised when data exists but is in invalid format."""
    pass


class ParameterMismatchError(ValidationError):
    """Raised when theory output doesn't match expected schema."""
    pass

# Add project paths for imports
_current_dir = Path(__file__).parent.absolute()
_simulations_root = _current_dir.parent
_project_root = _simulations_root.parent
sys.path.insert(0, str(_project_root))
sys.path.insert(0, str(_simulations_root))

# Import schema definitions
from simulations.validation.validation_schema_v16_2 import (
    ValidationStatus,
    BoundType,
    ExperimentalReference,
    ParameterValidationSpec,
    PARAMETER_VALIDATIONS,
    EXPERIMENTAL_DATA_PATHS,
    SIGMA_THRESHOLDS,
    get_validation_schema,
)


# ============================================================================
# Data Classes
# ============================================================================

@dataclass
class ValidationResult:
    """Result of validating a single parameter."""
    parameter_path: str
    name: str
    sector: str
    units: str
    predicted_value: Optional[float]
    experimental_value: Optional[float]
    uncertainty: Optional[float]
    sigma: Optional[float]
    status: ValidationStatus
    source_name: str
    bound_type: BoundType
    note: str = ""
    prediction_source: str = ""
    experimental_source_file: str = ""

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON export."""
        d = asdict(self)
        d['status'] = self.status.value
        d['bound_type'] = self.bound_type.value
        return d


@dataclass
class ValidationSummary:
    """Summary of all validation results."""
    total_parameters: int
    passed: int
    marginal: int
    tension: int
    failed: int
    missing: int
    global_chi_squared: float
    reduced_chi_squared: float
    degrees_of_freedom: int
    timestamp: str
    theory_version: str
    unitary_status: str

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON export."""
        return asdict(self)


# ============================================================================
# Main Validator Class
# ============================================================================

class OutputValidator:
    """
    Unified validator for Principia Metaphysica predictions.

    Reads all values from files - contains NO hardcoded predictions.
    Uses validation_schema_v16_2.py for schema definitions.
    """

    def __init__(self,
                 theory_output_path: Optional[str] = None,
                 experimental_data_dir: Optional[str] = None,
                 verbose: bool = True,
                 strict: bool = True):
        """
        Initialize the output validator.

        Args:
            theory_output_path: Path to theory_output.json (default: AutoGenerated/)
            experimental_data_dir: Path to experimental data directory
            verbose: Whether to print progress information
            strict: If True (default), raise exceptions on missing/invalid data.
                    If False, log warnings but continue with partial validation.
                    PRODUCTION BUILDS SHOULD ALWAYS USE strict=True.
        """
        self.verbose = verbose
        self.strict = strict

        # Set default paths
        if theory_output_path is None:
            self.theory_output_path = _project_root / "AutoGenerated" / "theory_output.json"
        else:
            self.theory_output_path = Path(theory_output_path)

        if experimental_data_dir is None:
            self.experimental_data_dir = _simulations_root / "data" / "experimental"
        else:
            self.experimental_data_dir = Path(experimental_data_dir)

        # Data storage
        self._theory_data: Optional[Dict[str, Any]] = None
        self._experimental_data: Dict[str, Dict[str, Any]] = {}
        self._validation_results: List[ValidationResult] = []
        self._summary: Optional[ValidationSummary] = None

    # -------------------------------------------------------------------------
    # Data Loading Methods
    # -------------------------------------------------------------------------

    def load_theory_output(self) -> Dict[str, Any]:
        """
        Load theory predictions from theory_output.json.

        Returns:
            Dictionary containing all theory output data

        Raises:
            FileNotFoundError: If theory_output.json doesn't exist
            json.JSONDecodeError: If file contains invalid JSON
        """
        if self._theory_data is not None:
            return self._theory_data

        if self.verbose:
            print(f"[LOAD] Reading theory output from: {self.theory_output_path}")

        if not self.theory_output_path.exists():
            raise FileNotFoundError(f"Theory output not found: {self.theory_output_path}")

        with open(self.theory_output_path, 'r', encoding='utf-8') as f:
            self._theory_data = json.load(f)

        if self.verbose:
            version = self._theory_data.get('metadata', {}).get('version', 'unknown')
            n_params = len(self._theory_data.get('parameters', {}))
            print(f"       Theory version: {version}, Parameters: {n_params}")

        return self._theory_data

    def load_experimental_data(self) -> Dict[str, Dict[str, Any]]:
        """
        Load all experimental data from JSON files.

        Returns:
            Dictionary mapping source names to experimental data

        Raises:
            MissingDataError: If required experimental file is missing (strict mode)
            InvalidDataError: If experimental file contains invalid JSON (strict mode)

        Note:
            In strict mode (default), missing or invalid files raise exceptions.
            In lenient mode, warnings are logged but validation continues.
        """
        if self._experimental_data:
            return self._experimental_data

        if self.verbose:
            print(f"[LOAD] Reading experimental data from: {self.experimental_data_dir}")

        missing_files = []
        invalid_files = []

        # Load files defined in schema
        for source_name, filename in EXPERIMENTAL_DATA_PATHS.items():
            filepath = self.experimental_data_dir / filename
            if filepath.exists():
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        self._experimental_data[source_name] = json.load(f)
                    if self.verbose:
                        print(f"       Loaded: {filename}")
                except json.JSONDecodeError as e:
                    invalid_files.append((filename, str(e)))
                    if self.verbose:
                        print(f"       [ERROR] Invalid JSON in {filename}: {e}")
            else:
                missing_files.append(filename)
                if self.verbose:
                    print(f"       [ERROR] File not found: {filename}")

        # In strict mode, fail fast on missing or invalid required files
        if self.strict:
            if missing_files:
                raise MissingDataError(
                    f"Required experimental data files not found: {', '.join(missing_files)}. "
                    f"Run 'python run_all_simulations.py' to generate required data files."
                )
            if invalid_files:
                details = "; ".join([f"{f}: {e}" for f, e in invalid_files])
                raise InvalidDataError(
                    f"Invalid JSON in experimental data files: {details}"
                )

        # Also try to load any additional JSON files in the directory
        if self.experimental_data_dir.exists():
            for json_file in self.experimental_data_dir.glob("*.json"):
                # Skip files already loaded via schema
                if json_file.name in EXPERIMENTAL_DATA_PATHS.values():
                    continue

                source_key = json_file.stem  # filename without extension
                if source_key not in self._experimental_data:
                    try:
                        with open(json_file, 'r', encoding='utf-8') as f:
                            self._experimental_data[source_key] = json.load(f)
                        if self.verbose:
                            print(f"       Loaded (extra): {json_file.name}")
                    except json.JSONDecodeError:
                        pass  # Skip invalid files

        return self._experimental_data

    # -------------------------------------------------------------------------
    # Data Access Methods
    # -------------------------------------------------------------------------

    def get_nested_value(self, data: Dict[str, Any], path: str) -> Any:
        """
        Get a nested value from a dictionary using dot notation.

        Args:
            data: Dictionary to traverse
            path: Dot-separated path (e.g., "parameters.cosmology.w0_derived.value")

        Returns:
            Value at the specified path, or None if not found

        Examples:
            >>> get_nested_value(data, "parameters.cosmology.w0_derived.value")
            -0.9583333
            >>> get_nested_value(data, "metadata.version")
            "16.2"
        """
        if not path:
            return None

        keys = path.split('.')
        current = data

        for key in keys:
            if isinstance(current, dict):
                if key in current:
                    current = current[key]
                else:
                    return None
            else:
                return None

        return current

    def get_theory_prediction(self, param_path: str) -> Tuple[Optional[float], str]:
        """
        Get a theory prediction value from theory_output.json.

        The theory_output.json stores parameters in a flat structure:
        "parameters": {
            "cosmology.w0_derived": {
                "value": -0.9583,
                "source": "...",
                ...
            }
        }

        Args:
            param_path: Parameter path (e.g., "parameters.cosmology.w0_derived.value")

        Returns:
            Tuple of (value, source_simulation) or (None, "") if not found
        """
        if self._theory_data is None:
            self.load_theory_output()

        params = self._theory_data.get('parameters', {})

        # Handle various path formats:
        # 1. "parameters.cosmology.w0_derived.value" -> "cosmology.w0_derived"
        # 2. "cosmology.w0_derived" -> "cosmology.w0_derived"
        # 3. "parameters.neutrino.theta_12_pred.value" -> "neutrino.theta_12_pred"

        param_key = param_path

        # Remove "parameters." prefix if present
        if param_key.startswith("parameters."):
            param_key = param_key[len("parameters."):]

        # Remove ".value" suffix if present
        if param_key.endswith(".value"):
            param_key = param_key[:-len(".value")]

        # Direct lookup in flat params structure
        if param_key in params:
            entry = params[param_key]
            value = entry.get('value')
            source = entry.get('source_simulation', entry.get('source', ''))
            return value, source

        # Try nested lookup (for paths like "geometry.alpha_inverse")
        # The param_key might be "geometry.alpha_inverse" but stored as that key directly
        # Or it might need nested traversal

        # Try as flat key with dot notation
        for key, entry in params.items():
            if key == param_key:
                value = entry.get('value')
                source = entry.get('source_simulation', entry.get('source', ''))
                return value, source

        # Try as direct nested path in the full data
        value = self.get_nested_value(self._theory_data, param_path)
        if value is not None:
            return value, ""

        # Try looking in parameters with just the last part of the path
        # e.g., "parameters.gauge.sin2_theta_W_pred.value" might map to "gauge.sin2_theta_W_pred"
        # or to "constants.sin2_theta_W_pred"
        parts = param_key.split('.')
        if len(parts) >= 2:
            # Try with just sector.param format
            short_key = '.'.join(parts[-2:]) if len(parts) > 2 else param_key
            if short_key in params:
                entry = params[short_key]
                value = entry.get('value')
                source = entry.get('source_simulation', entry.get('source', ''))
                return value, source

        # NOT FOUND - this is a critical error in strict mode
        # It means theory_output.json is missing a required prediction
        return None, ""

    def _assert_prediction_exists(self, param_path: str, value: Optional[float], spec_name: str) -> None:
        """
        Assert that a prediction exists in strict mode.

        Args:
            param_path: The parameter path that was searched
            value: The value that was found (or None)
            spec_name: Human-readable name for error messages

        Raises:
            ParameterMismatchError: If value is None and strict mode is enabled
        """
        if self.strict and value is None:
            raise ParameterMismatchError(
                f"Theory prediction not found for '{spec_name}' at path '{param_path}'. "
                f"Ensure run_all_simulations.py exports this parameter to theory_output.json. "
                f"Available parameters: {list(self._theory_data.get('parameters', {}).keys())[:10]}..."
            )

    def get_experimental_value(self,
                               source_file: str,
                               json_path: str,
                               uncertainty_path: Optional[str] = None
                               ) -> Tuple[Optional[float], Optional[float]]:
        """
        Get an experimental value and uncertainty from experimental data files.

        Args:
            source_file: Filename of the experimental data (e.g., "desi_2025_constraints.json")
            json_path: Path to the value within the JSON (e.g., "cosmological_parameters.w0.value")
            uncertainty_path: Optional path to uncertainty (e.g., "cosmological_parameters.w0.uncertainty_plus")

        Returns:
            Tuple of (value, uncertainty) or (None, None) if not found
        """
        if not self._experimental_data:
            self.load_experimental_data()

        # Find the correct data source
        source_key = None
        for key, filename in EXPERIMENTAL_DATA_PATHS.items():
            if filename == source_file:
                source_key = key
                break

        # Also try matching by filename stem
        if source_key is None:
            source_key = Path(source_file).stem

        if source_key not in self._experimental_data:
            return None, None

        exp_data = self._experimental_data[source_key]

        # Get the value
        value = self.get_nested_value(exp_data, json_path)

        # Get uncertainty
        uncertainty = None
        if uncertainty_path:
            uncertainty = self.get_nested_value(exp_data, uncertainty_path)

        # If uncertainty not found via path, try common patterns
        if uncertainty is None and value is not None:
            # Try getting uncertainty from same object
            base_path = '.'.join(json_path.split('.')[:-1])
            if base_path:
                uncertainty = self.get_nested_value(exp_data, f"{base_path}.uncertainty")
                if uncertainty is None:
                    uncertainty = self.get_nested_value(exp_data, f"{base_path}.uncertainty_plus")

        return value, uncertainty

    def _assert_experimental_exists(self, source_file: str, json_path: str,
                                     value: Optional[float], spec_name: str) -> None:
        """
        Assert that an experimental value exists in strict mode.

        Args:
            source_file: The experimental data file that was searched
            json_path: The JSON path that was searched
            value: The value that was found (or None)
            spec_name: Human-readable name for error messages

        Raises:
            MissingDataError: If value is None and strict mode is enabled
        """
        if self.strict and value is None:
            raise MissingDataError(
                f"Experimental value not found for '{spec_name}' at path '{json_path}' "
                f"in file '{source_file}'. Check that the experimental data file contains "
                f"this value at the specified path."
            )

    # -------------------------------------------------------------------------
    # Validation Methods
    # -------------------------------------------------------------------------

    def compute_sigma(self,
                      predicted: Optional[float],
                      experimental: Optional[float],
                      uncertainty: Optional[float]) -> Optional[float]:
        """
        Compute sigma deviation between prediction and experiment.

        Args:
            predicted: Theory prediction value
            experimental: Experimental measurement value
            uncertainty: 1-sigma uncertainty on experimental value

        Returns:
            Sigma deviation (|predicted - experimental| / uncertainty)
            or None if any input is None or uncertainty is zero
        """
        if predicted is None or experimental is None or uncertainty is None:
            return None

        if uncertainty <= 0:
            return None

        return abs(predicted - experimental) / uncertainty

    def _determine_status(self, sigma: Optional[float]) -> ValidationStatus:
        """Determine validation status from sigma value."""
        if sigma is None:
            return ValidationStatus.MISSING

        if sigma < SIGMA_THRESHOLDS["pass"]:
            return ValidationStatus.PASS
        elif sigma < SIGMA_THRESHOLDS["marginal"]:
            return ValidationStatus.MARGINAL
        elif sigma < SIGMA_THRESHOLDS["tension"]:
            return ValidationStatus.TENSION
        else:
            return ValidationStatus.FAIL

    def validate_parameter(self, spec: ParameterValidationSpec) -> ValidationResult:
        """
        Validate a single parameter against experimental data.

        Args:
            spec: Parameter validation specification from schema

        Returns:
            ValidationResult with all validation information

        Raises:
            ParameterMismatchError: If theory prediction not found (strict mode)
            MissingDataError: If experimental value not found (strict mode)
        """
        # Get theory prediction
        predicted, pred_source = self.get_theory_prediction(spec.prediction_path)

        # STRICT MODE: Assert prediction exists - fail fast, no silent fallbacks
        self._assert_prediction_exists(spec.prediction_path, predicted, spec.name)

        # Get experimental value
        exp_ref = spec.experimental_ref
        experimental, uncertainty = self.get_experimental_value(
            exp_ref.source_file,
            exp_ref.json_path,
            exp_ref.uncertainty_path
        )

        # STRICT MODE: Assert experimental value exists - fail fast, no silent fallbacks
        self._assert_experimental_exists(exp_ref.source_file, exp_ref.json_path,
                                          experimental, spec.name)

        # Override uncertainty if using theory uncertainty
        if spec.use_theory_uncertainty and spec.theory_uncertainty is not None:
            uncertainty = spec.theory_uncertainty

        # Compute sigma
        sigma = self.compute_sigma(predicted, experimental, uncertainty)

        # Determine status
        status = self._determine_status(sigma)

        # Handle missing prediction specially (only reached in lenient mode)
        if predicted is None:
            status = ValidationStatus.MISSING

        return ValidationResult(
            parameter_path=spec.prediction_path,
            name=spec.name,
            sector=spec.sector,
            units=spec.units,
            predicted_value=predicted,
            experimental_value=experimental,
            uncertainty=uncertainty,
            sigma=sigma,
            status=status,
            source_name=exp_ref.source_name,
            bound_type=exp_ref.bound_type,
            note=spec.note,
            prediction_source=pred_source,
            experimental_source_file=exp_ref.source_file
        )

    def validate_all_parameters(self) -> List[ValidationResult]:
        """
        Validate all parameters defined in the schema.

        Returns:
            List of ValidationResult objects for all parameters
        """
        # Ensure data is loaded
        self.load_theory_output()
        self.load_experimental_data()

        if self.verbose:
            print(f"\n[VALIDATE] Validating {len(PARAMETER_VALIDATIONS)} parameters...")
            print("-" * 70)

        self._validation_results = []

        for spec in PARAMETER_VALIDATIONS:
            result = self.validate_parameter(spec)
            self._validation_results.append(result)

            if self.verbose:
                status_icon = {
                    ValidationStatus.PASS: "[OK]",
                    ValidationStatus.MARGINAL: "[~]",
                    ValidationStatus.TENSION: "[!]",
                    ValidationStatus.FAIL: "[X]",
                    ValidationStatus.MISSING: "[?]",
                    ValidationStatus.INVALID: "[E]"
                }.get(result.status, "[?]")

                sigma_str = f"{result.sigma:.2f}sigma" if result.sigma else "N/A"
                print(f"  {status_icon} {result.name:<30} {sigma_str:>12} ({result.status.value})")

        return self._validation_results

    # -------------------------------------------------------------------------
    # Statistics and Summary
    # -------------------------------------------------------------------------

    def _compute_statistics(self) -> Tuple[float, float, int]:
        """
        Compute global chi-squared and reduced chi-squared.

        Returns:
            Tuple of (chi_squared, reduced_chi_squared, degrees_of_freedom)
        """
        # Only include measured parameters with valid sigma
        valid_results = [
            r for r in self._validation_results
            if r.sigma is not None
            and r.bound_type == BoundType.MEASURED
            and r.status != ValidationStatus.MISSING
        ]

        if not valid_results:
            return 0.0, 0.0, 0

        # Chi-squared = sum of sigma^2
        chi_squared = sum(r.sigma ** 2 for r in valid_results)

        # Degrees of freedom = number of predictions (PM has no free parameters)
        dof = len(valid_results)

        # Reduced chi-squared
        reduced_chi_squared = chi_squared / dof if dof > 0 else 0.0

        return chi_squared, reduced_chi_squared, dof

    def _determine_unitary_status(self) -> str:
        """Determine if theory is publication-ready."""
        if not self._validation_results:
            return "NOT_READY"

        counts = self._count_by_status()
        n_total = counts['total']

        if n_total == 0:
            return "NOT_READY"

        # Criteria for publication readiness:
        # - No failures (>3sigma deviations)
        # - Less than 20% in tension (2-3sigma)
        # - Reduced chi-squared < 2.0

        if counts[ValidationStatus.FAIL] > 0:
            return "NOT_READY"
        elif counts[ValidationStatus.TENSION] / n_total > 0.2:
            return "REVIEW_NEEDED"
        else:
            return "PUBLICATION_READY"

    def _count_by_status(self) -> Dict[str, int]:
        """Count results by validation status."""
        counts = {
            ValidationStatus.PASS: 0,
            ValidationStatus.MARGINAL: 0,
            ValidationStatus.TENSION: 0,
            ValidationStatus.FAIL: 0,
            ValidationStatus.MISSING: 0,
            ValidationStatus.INVALID: 0,
            'total': len(self._validation_results)
        }

        for r in self._validation_results:
            counts[r.status] = counts.get(r.status, 0) + 1

        return counts

    def generate_report(self) -> Dict[str, Any]:
        """
        Generate complete validation report.

        Returns:
            Dictionary containing all validation data ready for JSON export
        """
        if not self._validation_results:
            self.validate_all_parameters()

        # Compute statistics
        chi_sq, reduced_chi_sq, dof = self._compute_statistics()
        counts = self._count_by_status()
        unitary_status = self._determine_unitary_status()

        # Create summary
        self._summary = ValidationSummary(
            total_parameters=counts['total'],
            passed=counts[ValidationStatus.PASS],
            marginal=counts[ValidationStatus.MARGINAL],
            tension=counts[ValidationStatus.TENSION],
            failed=counts[ValidationStatus.FAIL],
            missing=counts[ValidationStatus.MISSING],
            global_chi_squared=chi_sq,
            reduced_chi_squared=reduced_chi_sq,
            degrees_of_freedom=dof,
            timestamp=datetime.now().isoformat(),
            theory_version=self._theory_data.get('metadata', {}).get('version', 'unknown'),
            unitary_status=unitary_status
        )

        # Build report
        report = {
            "generator": "output_validator_v16_2",
            "timestamp": datetime.now().isoformat(),
            "theory_source": str(self.theory_output_path),
            "experimental_sources": list(self._experimental_data.keys()),
            "schema_version": "16.2",
            "summary": self._summary.to_dict(),
            "sigma_table": [r.to_dict() for r in self._validation_results],
            "validation_schema": get_validation_schema()
        }

        if self.verbose:
            self._print_summary()

        return report

    def _print_summary(self) -> None:
        """Print formatted summary to console."""
        if self._summary is None:
            return

        print("\n" + "=" * 70)
        print(" VALIDATION SUMMARY")
        print("=" * 70)
        print(f"  Total Parameters:     {self._summary.total_parameters}")
        print(f"  Passed (<1sigma):     {self._summary.passed}")
        print(f"  Marginal (1-2sigma):  {self._summary.marginal}")
        print(f"  Tension (2-3sigma):   {self._summary.tension}")
        print(f"  Failed (>3sigma):     {self._summary.failed}")
        print(f"  Missing Data:         {self._summary.missing}")
        print("-" * 70)
        print(f"  Global Chi-Squared:   {self._summary.global_chi_squared:.2f}")
        print(f"  Reduced Chi-Squared:  {self._summary.reduced_chi_squared:.2f}")
        print(f"  Degrees of Freedom:   {self._summary.degrees_of_freedom}")
        print("-" * 70)
        print(f"  Publication Status:   {self._summary.unitary_status}")
        print("=" * 70)

    # -------------------------------------------------------------------------
    # Export Methods
    # -------------------------------------------------------------------------

    def export_to_json(self, output_path: Optional[str] = None) -> str:
        """
        Export validation report to JSON file.

        Args:
            output_path: Path for output file (default: AutoGenerated/validation_report.json)

        Returns:
            Path to written file
        """
        report = self.generate_report()

        if output_path is None:
            output_path = _project_root / "AutoGenerated" / "validation_report.json"
        else:
            output_path = Path(output_path)

        # Ensure directory exists
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        if self.verbose:
            print(f"\n[EXPORT] Validation report written to: {output_path}")

        return str(output_path)

    def export_to_text(self, output_path: Optional[str] = None) -> str:
        """
        Export validation table to formatted text file.

        Args:
            output_path: Path for output file

        Returns:
            Path to written file
        """
        if not self._validation_results:
            self.validate_all_parameters()

        if output_path is None:
            output_path = _project_root / "reports" / "validation_table.txt"
        else:
            output_path = Path(output_path)

        # Build text output
        lines = [
            "=" * 100,
            " PRINCIPIA METAPHYSICA - UNIFIED OUTPUT VALIDATOR v16.2",
            "=" * 100,
            f" Generated: {datetime.now().isoformat()}",
            f" Theory Source: {self.theory_output_path}",
            "",
            f"{'Parameter':<35} {'Predicted':>14} {'Experimental':>14} {'Sigma':>10} {'Status':>12}",
            "-" * 100,
        ]

        # Group by sector
        sectors: Dict[str, List[ValidationResult]] = {}
        for r in self._validation_results:
            if r.sector not in sectors:
                sectors[r.sector] = []
            sectors[r.sector].append(r)

        for sector, results in sorted(sectors.items()):
            lines.append(f"\n  [{sector.upper()}]")
            for r in results:
                pred_str = f"{r.predicted_value:.6g}" if r.predicted_value is not None else "N/A"
                exp_str = f"{r.experimental_value:.6g}" if r.experimental_value is not None else "N/A"
                sigma_str = f"{r.sigma:.2f}" if r.sigma is not None else "N/A"
                lines.append(f"  {r.name:<33} {pred_str:>14} {exp_str:>14} {sigma_str:>10} {r.status.value:>12}")

        # Summary
        if self._summary:
            lines.extend([
                "",
                "=" * 100,
                f" GLOBAL CHI-SQUARED:   {self._summary.global_chi_squared:.2f}",
                f" REDUCED CHI-SQUARED:  {self._summary.reduced_chi_squared:.2f}",
                f" DEGREES OF FREEDOM:   {self._summary.degrees_of_freedom}",
                f" PUBLICATION STATUS:   {self._summary.unitary_status}",
                "=" * 100,
            ])

        # Write file
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))

        if self.verbose:
            print(f"[EXPORT] Validation table written to: {output_path}")

        return str(output_path)


# ============================================================================
# Main Execution
# ============================================================================

def main():
    """Run the unified output validator."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Validate Principia Metaphysica predictions against experimental data"
    )
    parser.add_argument(
        '--theory-output', '-t',
        type=str,
        default=None,
        help="Path to theory_output.json"
    )
    parser.add_argument(
        '--experimental-dir', '-e',
        type=str,
        default=None,
        help="Path to experimental data directory"
    )
    parser.add_argument(
        '--output', '-o',
        type=str,
        default=None,
        help="Output path for validation_report.json"
    )
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help="Suppress verbose output"
    )
    parser.add_argument(
        '--text-output',
        type=str,
        default=None,
        help="Also export to text file"
    )

    args = parser.parse_args()

    print("\n" + "=" * 70)
    print(" PRINCIPIA METAPHYSICA - UNIFIED OUTPUT VALIDATOR v16.2")
    print("=" * 70)

    # Create validator
    validator = OutputValidator(
        theory_output_path=args.theory_output,
        experimental_data_dir=args.experimental_dir,
        verbose=not args.quiet
    )

    # Run validation
    validator.validate_all_parameters()

    # Export results
    output_path = validator.export_to_json(args.output)

    if args.text_output:
        validator.export_to_text(args.text_output)

    # Print final status
    if validator._summary:
        print(f"\n Validation Complete: {validator._summary.unitary_status}")
        print(f" Report saved to: {output_path}")

    print("=" * 70 + "\n")

    # Return exit code based on validation status
    if validator._summary:
        if validator._summary.unitary_status == "PUBLICATION_READY":
            return 0
        elif validator._summary.unitary_status == "REVIEW_NEEDED":
            return 1
        else:
            return 2
    return 1


if __name__ == "__main__":
    sys.exit(main())
