#!/usr/bin/env python3
"""
PRINCIPIA METAPHYSICA - Unified Repository Packaging Script
============================================================

This script creates clean, OFFLINE packages for Zenodo submission by:
1. Cloning a fresh copy of the repository (or using local)
2. Removing all unnecessary files (temp scripts, deprecated sims, etc.)
3. STRIPPING Firebase/Auth dependencies for offline use
4. Organizing into academic submission structure
5. Injecting OMEGA_SEAL verification
6. Generating SHA-256 checksums for all files and final ZIP
7. Creating validated ZIP archives (regular + FULL)

Usage:
    python tools/cleanup_and_package.py [options]

Options:
    --local     Use current repo instead of fresh clone (for testing)
    --full      Also create _FULL package with simulations
    --no-zip    Build directories only, skip ZIP creation
    --7z        Use 7z compression (requires 7-Zip installed)

Copyright (c) 2025-2026 Andrew Keith Watts. All rights reserved.

Dedicated To:
    My Wife: Elizabeth May Watts
    Our Messiah: Jesus Of Nazareth
"""

import os
import sys
import re
import shutil
import stat
import zipfile
import json
import hashlib
import subprocess
import tempfile
import argparse
from pathlib import Path
from datetime import datetime
from typing import Set, List, Dict, Tuple, Optional


def _remove_readonly(func, path, excinfo):
    """Error handler for shutil.rmtree on Windows read-only files."""
    try:
        os.chmod(path, stat.S_IWRITE)
        func(path)
    except Exception:
        pass  # Ignore cleanup errors


# ============================================================
# CONFIGURATION
# ============================================================

REPO_URL = "https://github.com/andrewkwatts-maker/PrincipiaMetaphysica.git"
REPO_BRANCH = "main"
VERSION = "16.2"
TITLE = f"Principia Metaphysica v{VERSION}"

SCRIPT_DIR = Path(__file__).parent
ROOT = SCRIPT_DIR.parent
OUTPUT_DIR = ROOT / "zenodo_submission"

# ============================================================
# FILES/PATTERNS TO REMOVE
# ============================================================

# Firebase/Auth files to DELETE for offline mode
FIREBASE_AUTH_FILES = [
    "*firebase*.js",
    "*firebase*.md",
    "*Firebase*.js",
    "*Firebase*.md",
    "auth-guard*.js",
    "auth-guard*.md",
    "secrets_config*",
    ".env*",
]

# Folders to always exclude
EXCLUDE_FOLDERS = [
    "__pycache__",
    ".git",
    ".github",
    ".claude",
    ".pytest_cache",
    "node_modules",
    "zenodo_submission",
    "zenodo_package",
]

# Deprecated simulation patterns
DEPRECATED_SIM_PATTERNS = [
    "*_v12_*.py",
    "*_v13_*.py",
    "*_v11_*.py",
    "*_v10_*.py",
]

# Temp file prefixes to remove
TEMP_PREFIXES = [
    "test-", "temp_", "batch", "verify_", "polish_",
    "update_", "migrate_", "add_", "check_", "detailed_",
]

# ============================================================
# ZENODO STRUCTURE
# ============================================================

ZENODO_STRUCTURE = {
    "01_Paper": {
        "description": "Complete interactive paper with formula rendering",
        "files": ["index.html", "validation.html", "Launch.bat", "serve.py"],
        "folders": ["css", "js", "fonts", "images", "components", "foundations", "Pages"],
    },
    "02_Simulations": {
        "description": "v16 simulation suite with validated scripts",
        "files": ["config.py", "run_all_simulations.py", "requirements.txt"],
        "folders": ["simulations/v16", "simulations/base", "simulations/validation"],
    },
    "03_Data": {
        "description": "Generated theory output and parameter database",
        "files": [],
        "folders": ["AutoGenerated", "data"],
    },
    "04_Documentation": {
        "description": "Architecture, provenance, and usage documentation",
        "files": ["README.md", "ARCHITECTURE.md", "PROVENANCE.md", "CITATION.cff", "LICENSE", "FORMULAS.md"],
        "folders": ["docs", "diagrams", "PROOFS"],
    },
    "05_Verification": {
        "description": "Omega Seal and formal proof manifests",
        "files": [],
        "folders": ["simulations/AutoGenerated"],
    },
    "06_Tests": {
        "description": "Validation and falsifiability test suite",
        "files": [],
        "folders": ["tests"],
    },
}

# ============================================================
# OFFLINE LOADER - Replaces Firebase
# ============================================================

OFFLINE_LOADER_JS = '''/**
 * PRINCIPIA METAPHYSICA - Offline Data Loader
 * ============================================
 * Replaces Firebase with local JSON file loading for offline use.
 * Copyright (c) 2025-2026 Andrew Keith Watts. All rights reserved.
 */

window.PM = window.PM || {};
window.PM_OFFLINE = true;

PM.offlineData = {
    formulas: null,
    parameters: null,
    sections: null,
    theory: null,
    loaded: false
};

PM.getBasePath = function() {
    const path = window.location.pathname;
    if (path.includes('/01_Paper/')) return '../03_Data/AutoGenerated/';
    if (path.includes('/Pages/')) return '../AutoGenerated/';
    return './AutoGenerated/';
};

PM.loadJSON = async function(filename) {
    const basePath = PM.getBasePath();
    try {
        const response = await fetch(basePath + filename);
        if (!response.ok) throw new Error(`HTTP ${response.status}`);
        return await response.json();
    } catch (error) {
        console.warn(`[PM] Could not load ${filename}:`, error.message);
        return null;
    }
};

PM.initOffline = async function() {
    if (PM.offlineData.loaded) return;
    console.log('[PM] Loading offline data...');

    const [formulas, parameters, sections, theory] = await Promise.all([
        PM.loadJSON('formulas.json'),
        PM.loadJSON('parameters.json'),
        PM.loadJSON('sections.json'),
        PM.loadJSON('theory_output.json')
    ]);

    PM.offlineData.formulas = formulas;
    PM.offlineData.parameters = parameters;
    PM.offlineData.sections = sections;
    PM.offlineData.theory = theory;
    PM.offlineData.loaded = true;

    console.log('[PM] Offline data loaded');
    window.dispatchEvent(new CustomEvent('pm-data-ready'));
};

PM.getFormula = function(id) {
    if (!PM.offlineData.formulas) return null;
    const formulas = PM.offlineData.formulas.formulas || PM.offlineData.formulas;
    return formulas.find(f => f.id === id || f.formulaId === id);
};

PM.getParameter = function(path) {
    if (!PM.offlineData.parameters) return null;
    const params = PM.offlineData.parameters.parameters || PM.offlineData.parameters;
    return params.find(p => p.path === path || p.id === path || p.symbol === path);
};

PM.get = function(key) {
    const param = PM.getParameter(key);
    if (param) return { value: param.value || param.pmPredicted, units: param.units, name: param.name };
    return null;
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', () => PM.initOffline());
} else {
    PM.initOffline();
}
'''

# ============================================================
# UTILITY FUNCTIONS
# ============================================================

def run_command(cmd: List[str], cwd: Optional[Path] = None) -> Tuple[int, str, str]:
    """Run shell command."""
    result = subprocess.run(cmd, cwd=cwd, capture_output=True, text=True)
    return result.returncode, result.stdout, result.stderr


def get_file_hash(filepath: Path) -> str:
    """Generate SHA-256 hash."""
    sha256 = hashlib.sha256()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            sha256.update(chunk)
    return sha256.hexdigest()


def format_size(size_bytes: int) -> str:
    """Format file size."""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    return f"{size_bytes / (1024 * 1024):.1f} MB"


# ============================================================
# CLONE & CLEAN FUNCTIONS
# ============================================================

def clone_fresh_repo(temp_dir: Path) -> Path:
    """Clone fresh repository."""
    print("\n" + "=" * 60)
    print("PHASE 1: Cloning Fresh Repository")
    print("=" * 60)

    clone_path = temp_dir / "PrincipiaMetaphysica"
    print(f"  Repository: {REPO_URL}")
    print(f"  Branch: {REPO_BRANCH}")

    cmd = ["git", "clone", "--depth", "1", "--branch", REPO_BRANCH, REPO_URL, str(clone_path)]
    returncode, stdout, stderr = run_command(cmd)

    if returncode != 0:
        raise RuntimeError(f"Git clone failed: {stderr}")

    print("  [OK] Cloned successfully")

    # Remove .git
    git_dir = clone_path / ".git"
    if git_dir.exists():
        shutil.rmtree(git_dir, onerror=_remove_readonly)
        print("  [OK] Removed .git folder")

    return clone_path


def clean_repo(repo_path: Path) -> Dict[str, int]:
    """Remove unnecessary files."""
    print("\n" + "=" * 60)
    print("PHASE 2: Cleaning Repository")
    print("=" * 60)

    stats = {"files": 0, "folders": 0, "bytes": 0}

    # Remove excluded folders
    for folder_name in EXCLUDE_FOLDERS:
        for folder in repo_path.rglob(folder_name):
            if folder.is_dir():
                size = sum(f.stat().st_size for f in folder.rglob("*") if f.is_file())
                stats["bytes"] += size
                shutil.rmtree(folder, onerror=_remove_readonly)
                stats["folders"] += 1
                print(f"  [DELETE] {folder.name}/")

    # Remove deprecated simulations
    sims_dir = repo_path / "simulations"
    if sims_dir.exists():
        for pattern in DEPRECATED_SIM_PATTERNS:
            for filepath in sims_dir.rglob(pattern):
                if "v16" not in str(filepath) and "base" not in str(filepath):
                    stats["bytes"] += filepath.stat().st_size
                    filepath.unlink()
                    stats["files"] += 1
                    print(f"  [DELETE] {filepath.name}")

    # Remove temp files from root
    for item in repo_path.iterdir():
        if item.is_file():
            if item.suffix in {".pyc", ".pyo", ".log", ".tmp", ".bak"}:
                stats["bytes"] += item.stat().st_size
                item.unlink()
                stats["files"] += 1
                continue

            if item.suffix == ".py" and any(item.name.startswith(p) for p in TEMP_PREFIXES):
                stats["bytes"] += item.stat().st_size
                item.unlink()
                stats["files"] += 1
                print(f"  [DELETE] {item.name}")

            if item.suffix == ".md" and any(x in item.name.upper() for x in ["BATCH", "SUMMARY", "REPORT", "STATUS"]):
                stats["bytes"] += item.stat().st_size
                item.unlink()
                stats["files"] += 1
                print(f"  [DELETE] {item.name}")

    print(f"\n  [OK] Removed {stats['files']} files, {stats['folders']} folders ({format_size(stats['bytes'])})")
    return stats


# ============================================================
# FIREBASE/AUTH STRIPPING (OFFLINE MODE)
# ============================================================

def strip_firebase_auth(package_path: Path) -> int:
    """Remove all Firebase/Auth files and references for offline use."""
    print("\n" + "=" * 60)
    print("PHASE 3: Stripping Firebase/Auth (Offline Mode)")
    print("=" * 60)

    deleted = 0

    # Delete Firebase/Auth files
    for pattern in FIREBASE_AUTH_FILES:
        for filepath in package_path.rglob(pattern):
            try:
                if filepath.is_file():
                    filepath.unlink()
                    deleted += 1
                    print(f"  [DELETE] {filepath.name}")
            except Exception as e:
                print(f"  [ERROR] {filepath}: {e}")

    # Create offline loader
    js_dir = package_path / "01_Paper" / "js"
    if js_dir.exists():
        loader_path = js_dir / "pm-offline-loader.js"
        with open(loader_path, 'w', encoding='utf-8') as f:
            f.write(OFFLINE_LOADER_JS)
        print(f"  [CREATE] pm-offline-loader.js")

    # Update HTML files to remove Firebase references and add offline loader
    html_updated = 0
    for html_file in package_path.rglob("*.html"):
        try:
            content = html_file.read_text(encoding='utf-8')
            original = content

            # Remove Firebase script tags
            content = re.sub(
                r'<script[^>]*firebase[^>]*>.*?</script>',
                '', content, flags=re.IGNORECASE | re.DOTALL
            )
            content = re.sub(
                r'<script[^>]*auth-guard[^>]*>.*?</script>',
                '', content, flags=re.IGNORECASE | re.DOTALL
            )

            # Remove auth-loading class from body
            content = content.replace('<body class="auth-loading">', '<body>')
            content = re.sub(r'(<body[^>]*) class="auth-loading"', r'\1', content)

            # Show main-content by default
            content = content.replace('id="main-content" style="display: none;"', 'id="main-content"')

            # Add offline loader if not present
            if 'pm-offline-loader.js' not in content:
                head_close = content.find('</head>')
                if head_close > 0:
                    inject = '\n    <script src="js/pm-offline-loader.js"></script>\n'
                    content = content[:head_close] + inject + content[head_close:]

            if content != original:
                html_file.write_text(content, encoding='utf-8')
                html_updated += 1

        except Exception as e:
            print(f"  [ERROR] {html_file}: {e}")

    print(f"\n  [OK] Deleted {deleted} Firebase/auth files")
    print(f"  [OK] Updated {html_updated} HTML files for offline use")
    return deleted


# ============================================================
# PACKAGE CREATION
# ============================================================

def create_package_structure(source_path: Path, output_path: Path) -> List[Dict]:
    """Create organized Zenodo package structure."""
    print("\n" + "=" * 60)
    print("PHASE 4: Creating Package Structure")
    print("=" * 60)

    if output_path.exists():
        shutil.rmtree(output_path, onerror=_remove_readonly)
    output_path.mkdir(parents=True)

    file_manifest = []

    for section_name, section_config in ZENODO_STRUCTURE.items():
        section_dir = output_path / section_name
        section_dir.mkdir(parents=True, exist_ok=True)
        print(f"\n  Creating {section_name}/")

        # Copy files
        for filename in section_config.get("files", []):
            source = source_path / filename
            if source.exists():
                dest = section_dir / source.name
                shutil.copy2(source, dest)
                file_manifest.append({
                    "path": f"{section_name}/{source.name}",
                    "size": dest.stat().st_size,
                    "sha256": get_file_hash(dest)
                })
                print(f"    [COPY] {filename}")

        # Copy folders
        for folder_path in section_config.get("folders", []):
            source = source_path / folder_path
            if source.exists():
                folder_name = Path(folder_path).name
                dest = section_dir / folder_name
                shutil.copytree(
                    source, dest,
                    ignore=shutil.ignore_patterns('__pycache__', '*.pyc', '.git', '.DS_Store')
                )
                print(f"    [COPY DIR] {folder_path}/")
                for f in dest.rglob("*"):
                    if f.is_file():
                        file_manifest.append({
                            "path": str(f.relative_to(output_path)),
                            "size": f.stat().st_size,
                            "sha256": get_file_hash(f)
                        })

    return file_manifest


def inject_omega_seal(source_path: Path, output_path: Path):
    """Copy Omega Seal verification files to the package."""
    print("\n  Injecting Omega Seal...")

    omega_seal_src = source_path / "simulations" / "AutoGenerated" / "OMEGA_SEAL.json"
    omega_hash_src = source_path / "simulations" / "AutoGenerated" / "OMEGA_SEAL.hash"

    verify_dir = output_path / "05_Verification"
    verify_dir.mkdir(parents=True, exist_ok=True)

    if omega_seal_src.exists():
        shutil.copy2(omega_seal_src, verify_dir / "OMEGA_SEAL.json")
        print(f"    [COPY] OMEGA_SEAL.json")

        if omega_hash_src.exists():
            shutil.copy2(omega_hash_src, verify_dir / "OMEGA_SEAL.hash")
            print(f"    [COPY] OMEGA_SEAL.hash")

            # Create verification manifest at root
            with open(output_path / "VERIFICATION_MANIFEST.txt", 'w') as f:
                with open(omega_hash_src, 'r') as src:
                    f.write("PRINCIPIA METAPHYSICA v16.2 - VERIFICATION MANIFEST\n")
                    f.write("=" * 50 + "\n\n")
                    f.write("OMEGA SEAL (72-Gate Master Hash):\n")
                    f.write(src.read())
                    f.write(f"\nExport Date: {datetime.now().isoformat()}\n")
                    f.write("\nStatus: Auth-Free, 72-Gate Verified, G2-Residue Locked.\n")
            print(f"    [CREATE] VERIFICATION_MANIFEST.txt")
    else:
        print("    [WARN] OMEGA_SEAL.json not found - run finalize_lockdown.py first")


def create_manifest(output_path: Path, file_manifest: List[Dict]) -> Dict:
    """Create package manifest."""
    print("\n" + "=" * 60)
    print("PHASE 5: Creating Manifest")
    print("=" * 60)

    total_size = sum(f["size"] for f in file_manifest)
    section_counts = {}
    for f in file_manifest:
        section = f["path"].split("/")[0] if "/" in f["path"] else "root"
        section_counts[section] = section_counts.get(section, 0) + 1

    manifest = {
        "title": TITLE,
        "version": VERSION,
        "author": "Andrew Keith Watts",
        "affiliation": "Independent Physics Researcher - Brisbane, Australia",
        "repository": "https://github.com/andrewkwatts-maker/PrincipiaMetaphysica",
        "created": datetime.now().isoformat(),
        "description": "A unified geometric framework deriving all 58 Standard Model parameters from a single G2 manifold.",
        "keywords": ["theoretical physics", "G2 manifold", "Standard Model", "gauge unification"],
        "statistics": {
            "total_files": len(file_manifest),
            "total_size_bytes": total_size,
            "total_size_human": format_size(total_size),
            "files_by_section": section_counts,
        },
        "files": file_manifest
    }

    manifest_path = output_path / "MANIFEST.json"
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)

    print(f"  Total files: {len(file_manifest)}")
    print(f"  Total size: {format_size(total_size)}")
    print("  [CREATE] MANIFEST.json")
    return manifest


def create_readme(output_path: Path) -> None:
    """Create Zenodo README."""
    readme = f"""# {TITLE} - Zenodo Submission

## A First-Principles Geometric Theory

A unified geometric framework deriving all 58 Standard Model parameters
from a single G2 manifold with minimal calibration (1 fitted parameter: alpha_GUT).

### Author
Andrew Keith Watts - Independent Physics Researcher, Brisbane, Australia

### Package Contents
| Folder | Description |
|--------|-------------|
| 01_Paper/ | Interactive paper (works offline) |
| 02_Simulations/ | v16 simulation suite |
| 03_Data/ | Theory output and parameters |
| 04_Documentation/ | Architecture and provenance |
| 05_Verification/ | Omega Seal certificates |
| 06_Tests/ | Validation test suite |

### Quick Start
1. Open `01_Paper/index.html` in browser
2. Run simulations: `cd 02_Simulations && python run_all_simulations.py`

### Key Results
- 58 parameters derived from G2 geometry
- 44/44 certificates LOCKED (100%)
- 40/72 gates verified (remaining are foundational axioms)
- 25/26 predictions within 1 sigma

### License
Copyright (c) 2025-2026 Andrew Keith Watts. All rights reserved.

---
*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
    with open(output_path / "README.md", 'w') as f:
        f.write(readme)
    print("  [CREATE] README.md")


def create_zip(output_path: Path, dest_path: Path, suffix: str = "") -> Tuple[Path, str]:
    """Create ZIP archive with SHA-256 checksum."""
    print("\n" + "=" * 60)
    print("PHASE 6: Creating ZIP Archive")
    print("=" * 60)

    date_str = datetime.now().strftime('%Y%m%d')
    zip_name = f"Principia_Metaphysica_v{VERSION}_{date_str}{suffix}.zip"
    zip_path = dest_path / zip_name

    if zip_path.exists():
        zip_path.unlink()

    count = 0
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=9) as zipf:
        for root_dir, dirs, files in os.walk(output_path):
            dirs[:] = [d for d in dirs if d != '__pycache__']
            for file in files:
                file_path = Path(root_dir) / file
                arc_name = file_path.relative_to(output_path)
                zipf.write(file_path, arc_name)
                count += 1

    # Compute SHA-256 hash
    sha256 = get_file_hash(zip_path)

    print(f"  Files: {count}")
    print(f"  Size: {format_size(zip_path.stat().st_size)}")
    print(f"  SHA-256: {sha256}")
    print(f"  [OK] Created {zip_name}")

    # Write checksum file
    checksum_path = dest_path / f"{zip_name}.sha256"
    with open(checksum_path, 'w') as f:
        f.write(f"{sha256}  {zip_name}\n")
    print(f"  [OK] Created {zip_name}.sha256")

    return zip_path, sha256


def create_7z(output_path: Path, dest_path: Path, suffix: str = "") -> Tuple[Optional[Path], Optional[str]]:
    """Create 7z archive with SHA-256 checksum."""
    print("\n  Creating 7z archive...")

    date_str = datetime.now().strftime('%Y%m%d')
    archive_name = f"Principia_Metaphysica_v{VERSION}_{date_str}{suffix}.7z"
    archive_path = dest_path / archive_name

    # Find 7z executable
    if sys.platform == 'win32':
        seven_zip_paths = [
            r"C:\Program Files\7-Zip\7z.exe",
            r"C:\Program Files (x86)\7-Zip\7z.exe",
            "7z.exe",
        ]
        seven_zip = None
        for path in seven_zip_paths:
            if os.path.exists(path) or shutil.which(path):
                seven_zip = path
                break
    else:
        seven_zip = shutil.which("7z") or shutil.which("7za")

    if not seven_zip:
        print("  [WARN] 7z not found - skipping 7z archive")
        return None, None

    if archive_path.exists():
        archive_path.unlink()

    try:
        result = subprocess.run(
            [seven_zip, "a", "-t7z", "-mx=9", "-mfb=273", "-ms", "-md=31",
             str(archive_path), str(output_path) + "/*"],
            capture_output=True,
            text=True
        )

        if result.returncode != 0:
            print(f"  [ERROR] 7z failed: {result.stderr}")
            return None, None

    except Exception as e:
        print(f"  [ERROR] 7z failed: {e}")
        return None, None

    sha256 = get_file_hash(archive_path)
    print(f"  Archive: {archive_name}")
    print(f"  Size: {format_size(archive_path.stat().st_size)}")
    print(f"  SHA-256: {sha256}")

    # Write checksum file
    checksum_path = dest_path / f"{archive_name}.sha256"
    with open(checksum_path, 'w') as f:
        f.write(f"{sha256}  {archive_name}\n")
    print(f"  [OK] Created {archive_name}.sha256")

    return archive_path, sha256


def validate_package(output_path: Path) -> Tuple[bool, List[str]]:
    """Validate package."""
    print("\n" + "=" * 60)
    print("PHASE 7: Validating Package")
    print("=" * 60)

    issues = []

    # Check sections exist
    for section in ZENODO_STRUCTURE.keys():
        section_path = output_path / section
        if not section_path.exists() or not any(section_path.iterdir()):
            issues.append(f"Missing/empty section: {section}")

    # Check no Firebase files
    for pattern in ["*firebase*", "*Firebase*", "auth-guard*"]:
        if list(output_path.rglob(pattern)):
            issues.append(f"Firebase/auth files still present")
            break

    # Check critical files
    critical = [
        "01_Paper/index.html",
        "02_Simulations/config.py",
        "03_Data/AutoGenerated/theory_output.json",
        "MANIFEST.json",
    ]
    for f in critical:
        if not (output_path / f).exists():
            issues.append(f"Missing: {f}")

    if issues:
        print("  ISSUES:")
        for issue in issues:
            print(f"    [!] {issue}")
        return False, issues

    print("  [OK] All checks passed")
    return True, []


# ============================================================
# FULL PACKAGE (includes simulations)
# ============================================================

def create_full_package(source_path: Path, output_dir: Path) -> Tuple[Path, str]:
    """Create FULL package with simulations included."""
    print("\n" + "=" * 60)
    print("CREATING FULL PACKAGE (with simulations)")
    print("=" * 60)

    full_output = output_dir.parent / "zenodo_submission_FULL"
    if full_output.exists():
        shutil.rmtree(full_output, onerror=_remove_readonly)

    # Copy the regular package
    shutil.copytree(output_dir, full_output)

    # Add full simulations directory
    sims_src = source_path / "simulations"
    sims_dst = full_output / "02_Simulations" / "simulations"

    if sims_src.exists():
        if sims_dst.exists():
            shutil.rmtree(sims_dst, onerror=_remove_readonly)
        shutil.copytree(
            sims_src, sims_dst,
            ignore=shutil.ignore_patterns('__pycache__', '*.pyc', '.git', '.DS_Store')
        )
        print(f"  [COPY] Full simulations directory")

    # Add core module
    core_src = source_path / "core"
    core_dst = full_output / "02_Simulations" / "core"
    if core_src.exists():
        shutil.copytree(
            core_src, core_dst,
            ignore=shutil.ignore_patterns('__pycache__', '*.pyc')
        )
        print(f"  [COPY] Core module")

    # Add scripts
    scripts_src = source_path / "scripts"
    scripts_dst = full_output / "02_Simulations" / "scripts"
    if scripts_src.exists():
        shutil.copytree(
            scripts_src, scripts_dst,
            ignore=shutil.ignore_patterns('__pycache__', '*.pyc')
        )
        print(f"  [COPY] Scripts directory")

    return full_output


# ============================================================
# MAIN
# ============================================================

def main():
    parser = argparse.ArgumentParser(description="Create Zenodo package")
    parser.add_argument("--local", action="store_true", help="Use local repo")
    parser.add_argument("--full", action="store_true", help="Also create FULL package with simulations")
    parser.add_argument("--no-zip", action="store_true", help="Skip ZIP creation")
    parser.add_argument("--7z", dest="use_7z", action="store_true", help="Also create 7z archive")
    parser.add_argument("--output", type=str, default=None, help="Output directory")
    args = parser.parse_args()

    print("=" * 60)
    print("PRINCIPIA METAPHYSICA - Unified Package Creator")
    print("=" * 60)
    print(f"Version: {VERSION}")
    print(f"Full Package: {args.full}")

    output_path = Path(args.output) if args.output else OUTPUT_DIR
    temp_dir = None

    if args.local:
        print(f"\nMode: LOCAL")
        source_path = ROOT
    else:
        print(f"\nMode: FRESH CLONE")
        temp_dir = Path(tempfile.mkdtemp(prefix="pm_zenodo_"))
        try:
            source_path = clone_fresh_repo(temp_dir)
            clean_repo(source_path)
        except Exception as e:
            print(f"[ERROR] {e}")
            if temp_dir and temp_dir.exists():
                shutil.rmtree(temp_dir, onerror=_remove_readonly)
            sys.exit(1)

    try:
        # Create regular package
        file_manifest = create_package_structure(source_path, output_path)

        # Strip Firebase
        strip_firebase_auth(output_path)

        # Inject Omega Seal
        inject_omega_seal(source_path, output_path)

        # Create manifest and readme
        create_manifest(output_path, file_manifest)
        create_readme(output_path)

        # Validate
        valid, issues = validate_package(output_path)

        # Create archives
        zip_path = None
        sha256 = None
        full_zip_path = None
        full_sha256 = None

        if not args.no_zip:
            # Regular ZIP
            zip_path, sha256 = create_zip(output_path, ROOT)

            # 7z if requested
            if args.use_7z:
                create_7z(output_path, ROOT)

            # FULL package if requested
            if args.full:
                full_output = create_full_package(source_path, output_path)
                full_zip_path, full_sha256 = create_zip(full_output, ROOT, "_FULL")

                if args.use_7z:
                    create_7z(full_output, ROOT, "_FULL")

        # Summary
        print("\n" + "=" * 60)
        print("SUMMARY")
        print("=" * 60)
        print(f"  Package: {output_path}")
        if zip_path:
            print(f"  ZIP: {zip_path}")
            print(f"  SHA-256: {sha256}")
        if full_zip_path:
            print(f"\n  FULL ZIP: {full_zip_path}")
            print(f"  FULL SHA-256: {full_sha256}")
        print(f"\n  Status: {'READY' if valid else 'NEEDS REVIEW'}")

    finally:
        if temp_dir and temp_dir.exists():
            try:
                shutil.rmtree(temp_dir, onerror=_remove_readonly)
            except Exception:
                pass  # Ignore cleanup errors


if __name__ == "__main__":
    main()
