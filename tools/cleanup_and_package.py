#!/usr/bin/env python3
"""
PRINCIPIA METAPHYSICA - Repository Cleanup & Zenodo Packaging
==============================================================

This script creates a clean Zenodo submission package by:
1. Cloning a fresh copy of the repository
2. Removing all unnecessary files (temp scripts, deprecated sims, etc.)
3. Organizing into academic submission structure
4. Generating SHA-256 manifest for verification
5. Creating validated ZIP archive

Usage:
    python tools/cleanup_and_package.py [--local]

    --local    Use current repo instead of fresh clone (for testing)

Copyright (c) 2025-2026 Andrew Keith Watts. All rights reserved.

Dedicated To:
    My Wife: Elizabeth May Watts
    Our Messiah: Jesus Of Nazareth
"""

import os
import sys
import shutil
import zipfile
import json
import hashlib
import subprocess
import tempfile
import argparse
from pathlib import Path
from datetime import datetime
from typing import Set, List, Dict, Tuple, Optional

# ============================================================
# CONFIGURATION
# ============================================================

# GitHub repository URL
REPO_URL = "https://github.com/andrewkwatts-maker/PrincipiaMetaphysica.git"
REPO_BRANCH = "main"

# Version info
VERSION = "16.2"
TITLE = f"Principia Metaphysica v{VERSION}"

# Output paths (relative to script location)
SCRIPT_DIR = Path(__file__).parent
ROOT = SCRIPT_DIR.parent
OUTPUT_DIR = ROOT / "zenodo_submission"

# ============================================================
# FILES TO INCLUDE IN ZENODO PACKAGE
# ============================================================

# Essential root files
ESSENTIAL_ROOT_FILES = {
    "principia-metaphysica-paper.html",
    "config.py",
    "run_all_simulations.py",
    "requirements.txt",
    "CITATION.cff",
    "README.md",
    "LICENSE",
    "LICENSE_PM.txt",
    "PROVENANCE.md",
    "ARCHITECTURE.md",
}

# Folders to include entirely
INCLUDE_FOLDERS = {
    "AutoGenerated",
    "components",
    "css",
    "data",
    "diagrams",
    "docs",
    "fonts",
    "foundations",
    "images",
    "js",
    "tests",
}

# Simulation folders to include (only v16 and base)
SIMULATION_INCLUDE = {
    "simulations/v16",
    "simulations/base",
    "simulations/AutoGenerated",
}

# Patterns to ALWAYS exclude
EXCLUDE_PATTERNS = {
    "__pycache__",
    "*.pyc",
    "*.pyo",
    ".git",
    ".github",
    ".claude",
    ".DS_Store",
    "*.log",
    "*.tmp",
    "*.bak",
    "node_modules",
    ".env",
    "secrets_config.py",
    "zenodo_submission",
    "zenodo_package",
    "*.zip",
}

# ============================================================
# ZENODO ACADEMIC STRUCTURE
# ============================================================

ZENODO_STRUCTURE = {
    "01_Paper": {
        "description": "Complete interactive paper with formula rendering",
        "files": [
            "principia-metaphysica-paper.html",
        ],
        "folders": [
            "css",
            "js",
            "fonts",
            "images",
            "components",
            "foundations",
        ],
    },
    "02_Simulations": {
        "description": "v16 simulation suite with 34 validated scripts",
        "files": [
            "config.py",
            "run_all_simulations.py",
            "requirements.txt",
        ],
        "folders": [
            "simulations/v16",
            "simulations/base",
        ],
    },
    "03_Data": {
        "description": "Generated theory output and parameter database",
        "files": [],
        "folders": [
            "AutoGenerated",
            "data",
        ],
    },
    "04_Documentation": {
        "description": "Architecture, provenance, and usage documentation",
        "files": [
            "README.md",
            "ARCHITECTURE.md",
            "PROVENANCE.md",
            "CITATION.cff",
            "LICENSE",
            "LICENSE_PM.txt",
        ],
        "folders": [
            "docs",
            "diagrams",
        ],
    },
    "05_Verification": {
        "description": "Wolfram certificates and formal proof manifests",
        "files": [],
        "folders": [
            "simulations/AutoGenerated",
        ],
    },
    "06_Tests": {
        "description": "Validation and falsifiability test suite",
        "files": [],
        "folders": [
            "tests",
        ],
    },
}

# ============================================================
# UTILITY FUNCTIONS
# ============================================================

def run_command(cmd: List[str], cwd: Optional[Path] = None) -> Tuple[int, str, str]:
    """Run a shell command and return (returncode, stdout, stderr)."""
    result = subprocess.run(
        cmd,
        cwd=cwd,
        capture_output=True,
        text=True
    )
    return result.returncode, result.stdout, result.stderr


def get_file_hash(filepath: Path) -> str:
    """Generate SHA-256 hash of a file."""
    sha256 = hashlib.sha256()
    with open(filepath, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b''):
            sha256.update(chunk)
    return sha256.hexdigest()


def format_size(size_bytes: int) -> str:
    """Format file size in human-readable format."""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    else:
        return f"{size_bytes / (1024 * 1024):.1f} MB"


def should_exclude(path: Path, base: Path) -> bool:
    """Check if a path should be excluded from the package."""
    rel_path = str(path.relative_to(base))
    name = path.name

    for pattern in EXCLUDE_PATTERNS:
        if pattern.startswith("*"):
            # Suffix match
            if name.endswith(pattern[1:]):
                return True
        elif pattern in rel_path or name == pattern:
            return True

    return False


# ============================================================
# CLONE & CLEAN FUNCTIONS
# ============================================================

def clone_fresh_repo(temp_dir: Path) -> Path:
    """Clone a fresh copy of the repository."""
    print("\n" + "=" * 60)
    print("PHASE 1: Cloning Fresh Repository")
    print("=" * 60)

    clone_path = temp_dir / "PrincipiaMetaphysica"

    print(f"  Repository: {REPO_URL}")
    print(f"  Branch: {REPO_BRANCH}")
    print(f"  Target: {clone_path}")

    # Clone with depth 1 for speed (we only need latest)
    cmd = [
        "git", "clone",
        "--depth", "1",
        "--branch", REPO_BRANCH,
        REPO_URL,
        str(clone_path)
    ]

    returncode, stdout, stderr = run_command(cmd)

    if returncode != 0:
        print(f"  [ERROR] Clone failed: {stderr}")
        raise RuntimeError(f"Git clone failed: {stderr}")

    print(f"  [OK] Repository cloned successfully")

    # Remove .git folder (not needed for Zenodo)
    git_dir = clone_path / ".git"
    if git_dir.exists():
        shutil.rmtree(git_dir)
        print(f"  [OK] Removed .git folder")

    return clone_path


def clean_cloned_repo(repo_path: Path) -> Dict[str, int]:
    """Remove unnecessary files from cloned repository."""
    print("\n" + "=" * 60)
    print("PHASE 2: Cleaning Cloned Repository")
    print("=" * 60)

    stats = {
        "files_removed": 0,
        "folders_removed": 0,
        "bytes_freed": 0,
    }

    # Remove folders that shouldn't be in Zenodo
    folders_to_remove = [
        ".github",
        ".claude",
        "__pycache__",
        ".pytest_cache",
        "node_modules",
        "zenodo_submission",
        "zenodo_package",
    ]

    for folder_name in folders_to_remove:
        for folder in repo_path.rglob(folder_name):
            if folder.is_dir():
                size = sum(f.stat().st_size for f in folder.rglob("*") if f.is_file())
                stats["bytes_freed"] += size
                shutil.rmtree(folder)
                stats["folders_removed"] += 1
                print(f"  [DELETE] {folder.relative_to(repo_path)}/")

    # Remove deprecated simulation versions
    sims_dir = repo_path / "simulations"
    if sims_dir.exists():
        deprecated_patterns = ["*_v12_*.py", "*_v13_*.py", "*_v11_*.py", "*_v10_*.py"]
        for pattern in deprecated_patterns:
            for filepath in sims_dir.rglob(pattern):
                # Keep v16 and base
                if "v16" not in str(filepath) and "base" not in str(filepath):
                    stats["bytes_freed"] += filepath.stat().st_size
                    filepath.unlink()
                    stats["files_removed"] += 1
                    print(f"  [DELETE] {filepath.relative_to(repo_path)}")

        # Remove deprecated/adhoc folders
        for subdir in ["deprecated", "adhoc"]:
            dep_dir = sims_dir / subdir
            if dep_dir.exists():
                size = sum(f.stat().st_size for f in dep_dir.rglob("*") if f.is_file())
                stats["bytes_freed"] += size
                shutil.rmtree(dep_dir)
                stats["folders_removed"] += 1
                print(f"  [DELETE] simulations/{subdir}/")

    # Remove temporary files from root
    temp_extensions = {".pyc", ".pyo", ".log", ".tmp", ".bak"}
    temp_prefixes = ["test-", "temp_", "batch", "verify_", "polish_", "update_", "migrate_", "add_"]

    for item in repo_path.iterdir():
        if item.is_file():
            # Remove by extension
            if item.suffix in temp_extensions:
                stats["bytes_freed"] += item.stat().st_size
                item.unlink()
                stats["files_removed"] += 1
                print(f"  [DELETE] {item.name}")
                continue

            # Remove temp Python scripts
            if item.suffix == ".py" and item.name not in ESSENTIAL_ROOT_FILES:
                for prefix in temp_prefixes:
                    if item.name.startswith(prefix):
                        stats["bytes_freed"] += item.stat().st_size
                        item.unlink()
                        stats["files_removed"] += 1
                        print(f"  [DELETE] {item.name}")
                        break

            # Remove temp markdown reports (not essential docs)
            if item.suffix == ".md" and item.name not in ESSENTIAL_ROOT_FILES:
                if any(x in item.name.upper() for x in ["BATCH", "SUMMARY", "REPORT", "STATUS"]):
                    stats["bytes_freed"] += item.stat().st_size
                    item.unlink()
                    stats["files_removed"] += 1
                    print(f"  [DELETE] {item.name}")

    # Clean __pycache__ recursively
    for pycache in repo_path.rglob("__pycache__"):
        if pycache.is_dir():
            shutil.rmtree(pycache)
            stats["folders_removed"] += 1

    # Remove .pyc files recursively
    for pyc in repo_path.rglob("*.pyc"):
        stats["bytes_freed"] += pyc.stat().st_size
        pyc.unlink()
        stats["files_removed"] += 1

    print(f"\n  [OK] Removed {stats['files_removed']} files")
    print(f"  [OK] Removed {stats['folders_removed']} folders")
    print(f"  [OK] Freed {format_size(stats['bytes_freed'])}")

    return stats


# ============================================================
# PACKAGE CREATION FUNCTIONS
# ============================================================

def create_zenodo_structure(source_path: Path, output_path: Path) -> List[Dict]:
    """Create the organized Zenodo package structure."""
    print("\n" + "=" * 60)
    print("PHASE 3: Creating Zenodo Package Structure")
    print("=" * 60)

    # Clean output directory
    if output_path.exists():
        shutil.rmtree(output_path)
    output_path.mkdir(parents=True)

    file_manifest = []

    for section_name, section_config in ZENODO_STRUCTURE.items():
        section_dir = output_path / section_name
        section_dir.mkdir(parents=True, exist_ok=True)

        print(f"\n  Creating {section_name}/")
        print(f"    {section_config['description']}")

        # Copy individual files
        for filename in section_config.get("files", []):
            source = source_path / filename
            if source.exists():
                dest = section_dir / source.name
                shutil.copy2(source, dest)
                print(f"    [COPY] {filename}")

                file_manifest.append({
                    "path": f"{section_name}/{source.name}",
                    "size": dest.stat().st_size,
                    "sha256": get_file_hash(dest)
                })
            else:
                print(f"    [SKIP] {filename} (not found)")

        # Copy folders
        for folder_path in section_config.get("folders", []):
            source = source_path / folder_path
            if source.exists():
                # Get the folder name (last component)
                folder_name = Path(folder_path).name
                dest = section_dir / folder_name

                shutil.copytree(
                    source, dest,
                    ignore=shutil.ignore_patterns('__pycache__', '*.pyc', '.git', '.DS_Store')
                )
                print(f"    [COPY DIR] {folder_path}/")

                # Add all files to manifest
                for f in dest.rglob("*"):
                    if f.is_file():
                        rel = f.relative_to(output_path)
                        file_manifest.append({
                            "path": str(rel),
                            "size": f.stat().st_size,
                            "sha256": get_file_hash(f)
                        })
            else:
                print(f"    [SKIP] {folder_path}/ (not found)")

    return file_manifest


def create_manifest(output_path: Path, file_manifest: List[Dict]) -> Dict:
    """Create the package manifest with metadata."""
    print("\n" + "=" * 60)
    print("PHASE 4: Creating Package Manifest")
    print("=" * 60)

    # Calculate totals
    total_size = sum(f["size"] for f in file_manifest)

    # Count by section
    section_counts = {}
    for f in file_manifest:
        section = f["path"].split("/")[0]
        section_counts[section] = section_counts.get(section, 0) + 1

    manifest = {
        "title": TITLE,
        "version": VERSION,
        "author": "Andrew Keith Watts",
        "affiliation": "Independent Physics Researcher - Brisbane, Australia",
        "repository": "https://github.com/andrewkwatts-maker/PrincipiaMetaphysica",
        "created": datetime.now().isoformat(),
        "description": (
            "A unified geometric framework deriving all 58 Standard Model parameters "
            "from a single G2 manifold with minimal calibration (1 fitted parameter: alpha_GUT)."
        ),
        "keywords": [
            "theoretical physics",
            "grand unified theory",
            "G2 manifold",
            "Standard Model",
            "gauge unification",
            "neutrino physics",
            "proton decay"
        ],
        "license": "All Rights Reserved (Non-commercial use permitted with attribution)",
        "statistics": {
            "total_files": len(file_manifest),
            "total_size_bytes": total_size,
            "total_size_human": format_size(total_size),
            "files_by_section": section_counts,
        },
        "sections": {
            name: config["description"]
            for name, config in ZENODO_STRUCTURE.items()
        },
        "files": file_manifest
    }

    manifest_path = output_path / "MANIFEST.json"
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)

    print(f"  Total files: {len(file_manifest)}")
    print(f"  Total size: {format_size(total_size)}")
    print(f"  [CREATE] MANIFEST.json")

    # Print section breakdown
    print("\n  Files by section:")
    for section, count in sorted(section_counts.items()):
        print(f"    {section}: {count} files")

    return manifest


def create_readme(output_path: Path) -> None:
    """Create the Zenodo README file."""
    readme_content = f"""# {TITLE} - Zenodo Submission

## A First-Principles Geometric Theory

A unified geometric framework deriving all 58 Standard Model parameters
from a single G2 manifold with minimal calibration (1 fitted parameter: alpha_GUT).

### Author
Andrew Keith Watts
Independent Physics Researcher - Brisbane, Australia

### Repository
https://github.com/andrewkwatts-maker/PrincipiaMetaphysica

### Package Contents

| Folder | Description |
|--------|-------------|
| **01_Paper/** | Complete interactive paper with formula rendering |
| **02_Simulations/** | v16 simulation suite (34 validated scripts) |
| **03_Data/** | Generated theory output and parameter database |
| **04_Documentation/** | Architecture, provenance, and usage docs |
| **05_Verification/** | Wolfram certificates and formal proofs |
| **06_Tests/** | Validation and falsifiability test suite |

### Quick Start

1. **View the paper**: Open `01_Paper/principia-metaphysica-paper.html` in a browser

2. **Run simulations**:
   ```bash
   cd 02_Simulations
   pip install -r requirements.txt
   python run_all_simulations.py
   ```

3. **Verify proofs**: The formal proofs in `05_Verification/` can be validated
   using Wolfram Language with the provided certificates.

### Key Results

- **58 parameters** derived from G2 geometry
- **34/34 simulations** pass validation (100%)
- **35 formal proofs** with Wolfram certificates
- **87.2% agreement** with experimental data (PDG 2024, NuFIT 6.0, DESI DR2)

### Falsifiable Predictions

1. **Proton decay**: tau_p = 8.15 x 10^34 years (Hyper-K testable)
2. **Neutrino mass sum**: Sum(m_nu) = 0.066 eV (DESI DR2 compatible)
3. **Dark matter sector**: 5 stable mirror particles

### License

Copyright (c) 2025-2026 Andrew Keith Watts. All rights reserved.
Non-commercial use permitted with attribution.

### Dedication

To my wife Elizabeth May Watts, and our Messiah Jesus of Nazareth.

---

*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""

    readme_path = output_path / "README.md"
    with open(readme_path, 'w') as f:
        f.write(readme_content)

    print("  [CREATE] README.md")


def create_zip_archive(output_path: Path, dest_path: Path) -> Path:
    """Create the final ZIP archive."""
    print("\n" + "=" * 60)
    print("PHASE 5: Creating ZIP Archive")
    print("=" * 60)

    timestamp = datetime.now().strftime('%Y%m%d')
    zip_name = f"Principia_Metaphysica_v{VERSION}_{timestamp}.zip"
    zip_path = dest_path / zip_name

    # Remove old ZIP if exists
    if zip_path.exists():
        zip_path.unlink()

    print(f"  Creating: {zip_name}")

    file_count = 0
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED, compresslevel=9) as zipf:
        for root_dir, dirs, files in os.walk(output_path):
            # Skip __pycache__
            dirs[:] = [d for d in dirs if d != '__pycache__']

            for file in files:
                file_path = Path(root_dir) / file
                arc_name = file_path.relative_to(output_path)
                zipf.write(file_path, arc_name)
                file_count += 1

    zip_size = zip_path.stat().st_size
    print(f"  Files archived: {file_count}")
    print(f"  Archive size: {format_size(zip_size)}")
    print(f"  [OK] Created {zip_name}")

    return zip_path


# ============================================================
# VALIDATION FUNCTIONS
# ============================================================

def validate_package(output_path: Path) -> Tuple[bool, List[str]]:
    """Validate the created package has all required components."""
    print("\n" + "=" * 60)
    print("PHASE 6: Validating Package")
    print("=" * 60)

    issues = []

    # Check all sections exist
    for section_name in ZENODO_STRUCTURE.keys():
        section_path = output_path / section_name
        if not section_path.exists():
            issues.append(f"Missing section: {section_name}")
        elif not any(section_path.iterdir()):
            issues.append(f"Empty section: {section_name}")

    # Check critical files
    critical_files = [
        "01_Paper/principia-metaphysica-paper.html",
        "02_Simulations/config.py",
        "02_Simulations/run_all_simulations.py",
        "02_Simulations/requirements.txt",
        "03_Data/AutoGenerated/theory_output.json",
        "04_Documentation/README.md",
        "MANIFEST.json",
        "README.md",
    ]

    for filepath in critical_files:
        full_path = output_path / filepath
        if not full_path.exists():
            issues.append(f"Missing critical file: {filepath}")

    # Check manifest is valid JSON
    manifest_path = output_path / "MANIFEST.json"
    if manifest_path.exists():
        try:
            with open(manifest_path) as f:
                manifest = json.load(f)

            # Verify file count matches
            actual_files = sum(1 for _ in output_path.rglob("*") if _.is_file())
            manifest_files = manifest.get("statistics", {}).get("total_files", 0)

            # Account for MANIFEST.json and README.md which are created after manifest
            if abs(actual_files - manifest_files) > 2:
                issues.append(f"File count mismatch: manifest={manifest_files}, actual={actual_files}")

        except json.JSONDecodeError as e:
            issues.append(f"Invalid MANIFEST.json: {e}")

    # Check v16 simulations exist
    v16_path = output_path / "02_Simulations" / "v16"
    if v16_path.exists():
        v16_files = list(v16_path.rglob("*.py"))
        if len(v16_files) < 10:
            issues.append(f"Too few v16 simulations: {len(v16_files)} (expected 30+)")
        else:
            print(f"  [OK] v16 simulations: {len(v16_files)} files")

    # Report results
    if issues:
        print("\n  VALIDATION ISSUES:")
        for issue in issues:
            print(f"    [!] {issue}")
        return False, issues
    else:
        print("  [OK] All validation checks passed")
        return True, []


# ============================================================
# MAIN ENTRY POINT
# ============================================================

def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Create clean Zenodo submission package for Principia Metaphysica"
    )
    parser.add_argument(
        "--local", action="store_true",
        help="Use current repo instead of fresh clone (for testing)"
    )
    parser.add_argument(
        "--output", type=str, default=None,
        help="Custom output directory for package"
    )
    args = parser.parse_args()

    print("=" * 60)
    print("PRINCIPIA METAPHYSICA - Zenodo Package Creator")
    print("=" * 60)
    print(f"Version: {VERSION}")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    output_path = Path(args.output) if args.output else OUTPUT_DIR

    if args.local:
        # Use current repo (for testing)
        print(f"\nMode: LOCAL (using current repository)")
        print(f"Source: {ROOT}")
        source_path = ROOT
        temp_dir = None
    else:
        # Clone fresh repo
        print(f"\nMode: FRESH CLONE")
        temp_dir = Path(tempfile.mkdtemp(prefix="pm_zenodo_"))
        print(f"Temp directory: {temp_dir}")

        try:
            source_path = clone_fresh_repo(temp_dir)
            clean_cloned_repo(source_path)
        except Exception as e:
            print(f"\n[ERROR] Failed to clone/clean: {e}")
            if temp_dir and temp_dir.exists():
                shutil.rmtree(temp_dir)
            sys.exit(1)

    try:
        # Create package structure
        file_manifest = create_zenodo_structure(source_path, output_path)

        # Create manifest and readme
        create_manifest(output_path, file_manifest)
        create_readme(output_path)

        # Create ZIP archive
        zip_path = create_zip_archive(output_path, ROOT)

        # Validate
        valid, issues = validate_package(output_path)

        # Summary
        print("\n" + "=" * 60)
        print("SUMMARY")
        print("=" * 60)
        print(f"  Package location: {output_path}")
        print(f"  ZIP archive: {zip_path}")
        print(f"  Validation: {'PASSED' if valid else 'FAILED'}")

        if valid:
            print("\n[OK] Zenodo package created successfully!")
            print(f"\nNext steps:")
            print(f"  1. Upload {zip_path.name} to Zenodo")
            print(f"  2. Fill in metadata (author, description, keywords)")
            print(f"  3. Publish to get DOI")
        else:
            print("\n[!] Package created with issues - review before upload")
            sys.exit(1)

    finally:
        # Cleanup temp directory
        if temp_dir and temp_dir.exists():
            print(f"\n  Cleaning up temp directory...")
            shutil.rmtree(temp_dir)


if __name__ == "__main__":
    main()
